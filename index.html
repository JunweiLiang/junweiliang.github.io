<html>
<head>
	<meta  http-equiv="Content-Type" content="text/html; charset=utf-8" />
	<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=1" />
	<link rel="stylesheet" type="text/css" href="utils/bootstrap.min.css"/>
  <script language="javascript" src="utils/jquery.min.js"></script>
	<script language="javascript" src="utils/bootstrap.min.js"></script>
  <!-- -->
  <link rel="stylesheet" type="text/css" href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css"/>
	<link rel="stylesheet" type="text/css" href="utils/cssReset.css"/>
  <link rel="stylesheet" type="text/css" href="utils/css_layout.css"/>
	<title>Prof. Junwei Liang / æ¢ä¿Šå«</title>
  <!-- æœç´¢å¼•æ“ä¼˜åŒ–stuff -->
	<meta name="description"
    content="Academic website for Junwei Liang. Dr. Junwei Liang is currently a tenure-track Assistant Professor at The Hong Kong University of Science and Technology (Guangzhou). He is also an affiliate assistant professor at HKUST computer science & engineering department. He was a senior researcher at Tencent Youtu Lab working on cutting-edge computer vision research and applications. Prior to that, he received his Ph.D. degree from Carnegie Mellon University, working with Prof. Alexander Hauptmann. He is the recipient of Baidu Scholarship and Yahoo Fellowship, and awarded Rising Star Award at the World AI Conference in 2020. He is the winner of several public safety video analysis competitions, including ASAPS and TRECVID ActEV. His work has helped and been reported by major news agencies like the Washington Post and New York Times. His research interests include human trajectory forecasting, action recognition, and large-scale computer vision and video analytics in general. His mission: develop AI technologies for social good.">
	<meta name="keywords" content="Junwei Liang,CMU,HKUST,HKUST-GZ,Professor,computer vision,PhD,æ¢ä¿Šå«,Carnegie Mellon University,The Hong Kong University of Science and Technology">

	<!-- Global site tag (gtag.js) - Google Analytics/ no use after 06/2024 -->
  <!--
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-156016426-1"></script>
  <script>
  	// for Google Analytics, for free!
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());
    gtag('config', 'UA-156016426-1');

  </script>
-->

</head>
<body>
<div id="sidebar">
  <img class='me' src="resources/me.jpeg"></img>
  <br/>
  <div class="info">
    <h2 class="name">Prof. Junwei Liang</h2>
    <h2 class="name_chinese">æ¢ä¿Šå«</h2>
    <h2 class="email">junweiliang1114@gmail.com</h2>
    <h2 class="email">junweiliang@hkust-gz.edu.cn</h2>
    <h2 class="email">HKUST (Guangzhou) / HKUST</h2>
    <h2 class="email">Office: E4-304</h2>
    <h2 class="link">
      <a style="font-size: 14px; color: yellow; font-weight: bold;" href="https://scholar.google.com/citations?hl=en&user=bMedjfUAAAAJ">Google Scholar</a>
    </h2>
    <h2 class="link">
      <a href="https://www.semanticscholar.org/author/Junwei-Liang/1915796">[Semantic Scholar]</a>
      <a href="https://www.researchgate.net/profile/Junwei_Liang3">[Research Gate]</a>
    </h2>
    <h2 class="link">
      <a href="https://github.com/JunweiLiang">[Github]</a>
      <a href="https://www.linkedin.com/in/junweiliang/">[LinkedIn]</a>
      <a href="https://www.youtube.com/channel/UC-z7ZWp8Rbu2xhxnbAL_bRQ">[Youtube]</a>
    </h2>
    <h2 class="link">
      <a href="https://www.zhihu.com/people/junwei-liang-50">[çŸ¥ä¹]</a>
      <a href="https://www.xiaohongshu.com/user/profile/62c3a783000000001b02b099">[å°çº¢ä¹¦]</a>
      <a href="https://twitter.com/JunweiLiangCMU">[Twitter/X]</a>
    </h2>
    <h2 class="link">
      <a style="font-size: 14px; color: yellow; font-weight: bold;" href="https://precognition.team/">Precognition Lab</a>
    </h2>

    <!--
    <a class="quickLink" href="https://medium.com/@junweil">
      <img class='medium' style="" src="resources/medium.png"></img>
    </a>
    <a class="quickLink" href="https://dblp.org/pers/hd/l/Liang_0001:Junwei">
      <img class='dblp' style="height:20px" src="resources/dblp.png"></img>
    </a>
    <a class="quickLink" href="http://aminer.cn/profile/junwei-liang/562cb48c45cedb3398c9e13b">
      <img class='aminer' style="height:20px;width: 50px;margin-top:4px" src="resources/aminer.png"></img>
    </a>
    <a class="quickLink" href="https://g.co/kgs/gTWf5W">
      <img class='aminer' name="Google knowledge graph" style="height:30px;width: 30px;margin-top:0px" src="resources/gkg.png"></img>
    </a>-->

  </div>
  <div id="navigation">
    <a class="nav_item" href="./index.html">
      <i class="icon icon-home icon-white"></i> &nbsp; About
    </a>
    <a class="nav_item" href="./projects.html#projects">
      <i class="icon icon-th-large icon-white"></i> &nbsp; Projects
    </a>
    <a class="nav_item" href="./projects.html#publications">
      <i class="icon icon-file icon-white"></i> &nbsp; Publications
    </a>
    <a class="nav_item" href="./teaching.html#teaching">
      <i class="icon icon-user icon-white"></i> &nbsp; Teaching / Talks
    </a>
    <a class="nav_item" href="./index.html#awards">
      <i class="icon icon-bookmark icon-white"></i> &nbsp; Honors / Awards
    </a>
    <a class="nav_item" href="./index.html#media">
      <i class="icon icon-volume-up icon-white"></i> &nbsp; Selected Media
    </a>
    <a class="nav_item" href="./awesome.html">
      <i class="icon icon-list icon-white"></i> &nbsp; Awesome Lists
    </a>
    <a class="nav_item" href="./letter.html">
      <i class="icon icon-pencil icon-white"></i> &nbsp; Letter
    </a>

  </div>
</div>

<div id="main">
  <div class="title">
    <a class="title_link" id="bio" href="#bio">Bio</a>
    <!--<img src="https://visitor-badge.glitch.me/badge?page_id=JunweiLiang.JunweiLiang&right_color=green" alt=""/>-->
    <img src="https://vbr.wocr.tk/badge?page_id=JunweiLiang.JunweiLiang&right_color=green" alt=""/>

    <img id="logo" src="resources/hkustgz-logo.jpg"></img>
  </div>
  <div class="content">
    I am an Assistant Professor in the <a href="https://facultyprofiles.hkust-gz.edu.cn/thrust-faculties?code=10011A10000000000H28">AI thrust</a> at <a href="https://www.hkust-gz.edu.cn/">The Hong Kong University of Science and Technology (Guangzhou campus)</a>.
    I lead the <a href="https://precognition.team/">Precognition Lab</a>.
    I am also an Affiliate Assistant Professor at <a href="https://cse.hkust.edu.hk/admin/people/faculty/?c=affiliate_gz">HKUST computer science & engineering department</a>.
    I obtained my Ph.D. in 2021 from Carnegie Mellon University, advised by <a href="https://scholar.google.com/citations?user=Py54GcEAAAAJ&hl=en">Alexander Hauptmann</a>.

    <div class="linebreak"></div>
    <!--
    Prior to joining HKUST-GZ, I was a senior researcher at Tencent Youtu Lab working with <a href="https://scholar.google.com/citations?user=Ljk2BvIAAAAJ&hl=en">Chunhua Shen</a>. I completed my Ph.D. in 2021 from Carnegie Mellon University, advised by <a href="https://scholar.google.com/citations?user=Py54GcEAAAAJ&hl=en">Alexander Hauptmann</a>.
    My Ph.D. and research work were mostly funded by <a href="https://www.iarpa.gov/research-programs/diva">IARPA</a>, <a href="https://www.nist.gov/ctl/pscr/real-time-video-analytics-situation-awareness">NIST</a> and <a href="https://nsf.gov/awardsearch/showAward?AWD_ID=1650994&HistoricalAwards=false">NSF</a> grants.
    I was a research intern at Google AI multiple times and collaborated with <a href="https://scholar.google.com/citations?user=jIKjjSYAAAAJ&hl=en">Lu Jiang</a>, <a href="https://scholar.google.com/citations?user=S-hBSfIAAAAJ&hl=en">Liangliang Cao</a>, <a href="https://scholar.google.com/citations?user=feX1fWAAAAAJ">Jia Li</a> and <a href="https://scholar.google.com/citations?user=MxxZkEcAAAAJ&hl=en">Kevin Murphy</a>.
    I obtained my undergraduate degree from <a href="https://www.ruc.edu.cn/en">RUC</a>, advised by <a href="https://scholar.google.com/citations?user=8UkYbCMAAAAJ&hl=zh-CN">Qin Jin</a>.

    <div class="linebreak"></div>
    -->

    <!--
    <span style="color: red">I have multiple <strike>fully-funded Ph.D.</strike> and <strike>research assistant/intern positions available.</strike> (Our lab currently has around 10 people, and I want to ensure that everyone is producing meaningful research before I consider expanding the team.) </span> Please checkout potential <a href="./projects.html">projects</a> and <a href="./letter.html">this letter</a> if you are interested in joining our lab. æ‹›ç”Ÿæ–‡: [<a href="https://zhuanlan.zhihu.com/p/562523740">çŸ¥ä¹</a>] [<a href="https://www.xiaohongshu.com/discovery/item/6323ed9e00000000110142ad">å°çº¢ä¹¦</a>]
    -->

     <span style="color: red">
      I am looking for 1-2 PhD student for Fall 2026 to work on Embodied AI.
      I am likely to choose from current MPhil students in my lab.
      But send me your resume if you have one or more first-author papers published at CoRL/RAL/TRO (or NeurIPS/CVPR/ICCV/ICRA/IROS) and is interested in robot learning-based embodied AI.
      </span>


     Please checkout our <a href="https://precognition.team">lab resources</a> and <a href="./letter.html">this letter</a>. (æ‡’å¾—æ›´æ–°çš„)æ‹›ç”Ÿæ–‡: [<a href="https://zhuanlan.zhihu.com/p/562523740">çŸ¥ä¹</a>] [<a href="https://www.xiaohongshu.com/discovery/item/6323ed9e00000000110142ad">å°çº¢ä¹¦</a>]

    <div class="linebreak"></div>

    <!--
    For fellow assistant professors and Ph.D. students, I have gathered some <span style="font-weight: bold">awesome lists</span> of resources that may be useful for your success. See <a href="./awesome.html">here</a> and feel free to contribute on Github. [<a href="https://wx.zsxq.com/mweb/views/topicdetail/topicdetail.html?topic_id=411558545282218&group_id=142181451122&inviter_id=582155511885254">è¢«CVerç›—äº†hhh</a>]
  -->


  </div>

  <div class="subtitle">
    <a class="title_link" id="group" href="#group">Precognition Lab</a>
    (Website: <a href="https://precognition.team">precognition.team</a>)
    -
    <a href="https://precognition.team/index.html#publications">Publications</a>
  </div>

  <div class="content">
    Our research lab, the Precognition Lab (æ™ºèƒ½æ„ŸçŸ¥ä¸é¢„æµ‹å®éªŒå®¤), is interested in building human-level <span style="font-weight: bold;">Embodied AI</span> systems that can effectively perceive, reason, and interact with the real-world for the good of humans.
    Here is an up-to-date <a href="https://precognition.team/projects.html#roadmap">research roadmap</a>.
    Here are our on-going or finished <a href="./projects.html">research grants</a>.



    <div class="linebreak"></div>

    Our lab's computing resources include <b>36 RTX 3090/4090/L40 GPUs and a cluster of 24 A6000 GPUs</b> with a 100TB NAS. See <a href="https://www.linkedin.com/feed/update/urn:li:activity:7099940517966200832/">this post</a>.
    And we have multiple mobile platforms with robot arms and dex hands:

    <div class="linebreak"></div>

    <!-- <img src="resources/robot1.png" style="height:300px;margin:20px 30px 0 0"></img> -->
    <img src="resources/g1+go2w.jpg" style="height:300px;margin:20px 10px 20px 0"></img>
    <img src="resources/lab_small.jpeg" style="height:300px;margin:20px 10px 20px 0"></img> <br/>
    <img src="resources/corl_omniperception.gif" style="height:300px;margin:20px 10px 20px 0"></img>
    <img src="resources/aliengo_tennis.gif" style="height:300px;margin:20px 10px 20px 0"></img>
    <img src="resources/corl_glover++.gif" style="height:300px;margin:20px 10px 20px 0"></img>
    <!--<img src="resources/robotdoghand_open_door.gif" style="height:300px;margin:20px 10px 20px 0"></img>-->
    <!--<img src="resources/dex_handaliengo_grab_things.gif" style="height:300px;margin:20px 10px 20px 0"></img>-->
    <img src="resources/go2_arm.png" style="height:300px;margin:20px 10px 20px 0"></img>
    <img src="resources/robot3.png" style="height:300px;margin:20px 10px 20px 0"></img>

    <div class="linebreak"></div>

    If you want to meet, check out <a href="https://calendar.google.com/calendar/embed?src=junweiliang1114%40gmail.com&ctz=Asia%2FShanghai" target="_blank">my public calendar</a> first and propose a meeting via email.

  </div>

  <div class="title">
    <a class="title_link" id="news" href="#news">News</a>
  </div>

  <div class="content">
    <ul>
      <li>
        <span class="label label-info">12/2025</span> ASCENT is accepted by IEEE Robotics and Automation Letters (RA-L)!
        [<a href="https://zeying-gong.github.io/projects/ascent/">Project Page</a>]
        [<a href="https://arxiv.org/abs/2505.23019">Paper</a>]
        [<a href="https://github.com/Zeying-Gong/ascent">Code and model</a>]
      </li>
      <li>
        <span class="label label-info">12/2025</span> Our Self-driving VLA Survey is released and reported by others!
        [<a href="https://mp.weixin.qq.com/s/ZZnYSzwJTVDwb-QQpv7xgw">ã€VLAç»¼è¿°ã€‘æ¸¯ç§‘æ²ˆåŠ­åŠ¼æ¢ä¿Šå«NTUåˆ˜å­çº¬ç­‰å›¢é˜Ÿè‡ªåŠ¨é©¾é©¶VLAé‡ç£…ç»¼è¿°ï¼</a>]
        [<a href="https://arxiv.org/abs/2512.16760">Paper</a>]
      </li>
      <li>
        <span class="label label-info">12/2025</span> åœ¨å¤§æ¹¾åŒºç§‘å­¦è®ºå›-å…·èº«æ™ºèƒ½åˆ†è®ºå›è¿›è¡Œä¸»é¢˜æ¼”è®²å¹¶æ¼”ç¤ºå®éªŒå®¤Demo
        [<a href="https://mp.weixin.qq.com/s/4QT0VK9muqHCpobvZ3pHBA">æ¸¯ç§‘å¤§ï¼ˆå¹¿å·ï¼‰ä¸»åŠï¼2025å¤§æ¹¾åŒºç§‘å­¦è®ºå›å…·èº«æ™ºèƒ½åˆ†è®ºå›éš†é‡ä¸¾è¡Œ</a>]
        [<a href="https://www.xiaohongshu.com/explore/69342096000000001d039dca?xsec_token=AB-TJgirWrcOexYY3-m5D0DxRkncoe3hzQLZdI8h7vFQ0=&xsec_source=pc_user">å°çº¢ä¹¦</a>]
      </li>
      <li>
        <span class="label label-info">10/2025</span> 21ä¸–çºªè´¢ç»ä¸“è®¿
        [<a href="https://m.21jingji.com/article/20251208/herald/7d0d4108c457bb3abf676a1734beb3d6.html?t=772148">â€œæ¢ä¿Šå«ï¼šç»™å¤§æ¨¡å‹å®‰è£…â€œä¸€åŒæ‰‹â€è¦ç»å†å‡ æ­¥ï¼Ÿâ€</a>]
        [<a href="https://www.bilibili.com/video/BV1qV2SBSEoi/">Bç«™</a>]
      </li>
      <li>
        <span class="label label-info">09/2025</span>
        GLOVER accepted at CoRL 2025 GenPriors Workshop and received <span style="font-weight:bold;color:red">Best Paper awardğŸ¥‡</span>
        [<a href="https://teleema.github.io/projects/GLOVER/">Project page</a>]
        [<a href="https://x.com/JunweiLiangCMU/status/1972109244108623987">Twitter</a>]
        [<a href="https://www.xiaohongshu.com/explore/68d7a6a00000000012030773?app_platform=ios&app_version=9.1.2&share_from_user_hidden=true&xsec_source=app_share&type=normal&xsec_token=CBiPKCZngxg470muJsud7g2j4JtmYQtJ35q8GndUH3grk=&author_share=1&xhsshare=WeixinSession&shareRedId=ODhFM0Q8PDw2NzUyOTgwNjdFOTlGNT9C&apptime=1758963375&share_id=c61d61af7751472aa32764b87b05d9db">å°çº¢ä¹¦</a>]
      </li>
      <li>
        <span class="label label-info">09/2025</span>
        Two papers accepted at <span style="font-weight:bold;">NeurIPS 2025</span>.
        [<a href="https://jiaming-zhou.github.io/AGNOSTOS/">Cross-task manipulation benchmark, AGNOSTOS</a>]
        [<a href="https://project-3eed.github.io/">Ground Everything Everywhere in 3D</a>]
      </li>
      <li>
        <span class="label label-info">09/2025</span> å¹¿ä¸œçœå¼€å­¦ç¬¬ä¸€è¯¾ï¼šçœæ•™è‚²å…é‚€è¯·é¦™æ¸¯ç§‘æŠ€å¤§å­¦ï¼ˆå¹¿å·ï¼‰äººå·¥æ™ºèƒ½å­¦åŸŸåŠ©ç†æ•™æˆæ¢ä¿Šå«åšäººå·¥æ™ºèƒ½ä¸æœºå™¨äººäº§ä¸šç›¸å…³æŠ€æœ¯çŸ¥è¯†ç§‘æ™®å®£è®²
        [<a href="https://gdii.gd.gov.cn/tpxw987/content/post_4775401.html">å¹¿ä¸œçœå·¥ä¿¡å…</a>]

        [<a href="https://www.youtube.com/watch?v=VuL3wP8fuRQ">å¹¿ä¸œç»æµç§‘æ•™é¢‘é“ï¼šå¹¿ä¸œå¼€å­¦ç¬¬ä¸€è¯¾</a>]
        [<a href="https://cj.sina.com.cn/articles/view/2131593523/7f0d893302001lhcg?froms=ggmp">ç¾ŠåŸæ™šæŠ¥</a>]
      </li>
      <li>
        <span class="label label-info">08/2025</span>
        Two papers accepted at <span style="font-weight:bold;">CoRL 2025</span> <span style="font-weight:bold;color:red">(5%å½•å–ç‡ï¼Œå¤§æ¹¾åŒºå”¯ä¸€çš„Oral paper)</span>
        [<a href="https://acodedog.github.io/OmniPerceptionPages/">LiDAR-based Locomotion Policy</a>]
        [<a href="https://mp.weixin.qq.com/s/7gyV4iSF1G4sSyuoDRRMDg">æ·±è“å…·èº«æ™ºèƒ½æŠ¥é“</a>]
        [<a href="https://teleema.github.io/projects/GLOVER++/">GLOVE++: Open-Vocab Affordance</a>]
        [<a href="https://mp.weixin.qq.com/s/r9JvBsPf9jBQqiRe2UXVgQ?scene=1">æ™ºçŒ©çŒ©Robotè§£è¯»</a>]
      </li>
      <li>
        <span class="label label-info">07/2025</span> å¹¿å·æ—¥æŠ¥ä¸“è®¿
        [<a href="https://huacheng.gz-cmc.com/pages/2025/07/21/SF14148463618aaebf888241f4a893e0.html">â€œèƒ½ä¸»åŠ¨ä¸ºè¡Œäººè®©è·¯çš„æœºå™¨äººï¼Œæ‰æ˜¯å¥½æœºå™¨äººâ€</a>]
        [<a href="https://mp.weixin.qq.com/s/Ym9UxqPLb6OSZxrtxCeHqw?scene=1">ã€Šå¹¿å·æ—¥æŠ¥ Â· ç§‘æŠ€å‘¨åˆŠã€‹æ­ç§˜æ¸¯ç§‘å¤§ï¼ˆå¹¿å·ï¼‰äººå½¢æœºå™¨äººâ€œå¤§è„‘â€</a>]
      </li>
      <li>
        <span class="label label-info">07/2025</span>
        Make a humanoid robot demo with manipulation for the university.
        æ”¯æŒæ¸¯ç§‘å¹¿2025çº§æœ¬ç§‘ç”Ÿå½•å–æ´»åŠ¨ï¼Œäººå½¢æœºå™¨äººé€’é€å½•å–é€šçŸ¥ä¹¦
        [<a href="https://mp.weixin.qq.com/s/jIJfb1G1MPbs-GTfKwNhCQ?scene=1">ç§‘æŠ€èµ‹èƒ½è¿æ–° æ¸¯ç§‘å¤§ï¼ˆå¹¿å·ï¼‰ä¸¾åŠæœ¬ç§‘æ–°ç”Ÿä»£è¡¨è§é¢æ´»åŠ¨</a>]
        [å¾®ä¿¡è§†é¢‘å·ï¼šé¦™æ¸¯ç§‘æŠ€å¤§å­¦ï¼ˆå¹¿å·ï¼‰- ç›´å‡»æ¸¯ç§‘å¤§ï¼ˆå¹¿å·ï¼‰ç¡¬æ ¸æœ¬ç§‘æ–°ç”Ÿä»£è¡¨è§é¢ä¼šç°åœºï¼]
        [<a href="https://www.xiaohongshu.com/explore/687c52b5000000000b02d825?source=webshare&xhsshare=pc_web&xsec_token=ABrVSHQdRMREeTYR1Q0zKAOpzcRqmEAnJF5Rf1SCtAEvk=&xsec_source=pc_share">Demo Video - å°çº¢ä¹¦</a>]
        [å¾®ä¿¡è§†é¢‘å·ï¼šç¾ŠåŸæ™šæŠ¥ - æ¸¯ç§‘å¤§ï¼ˆå¹¿å·ï¼‰æœ¬ç§‘æ–°ç”Ÿä»£è¡¨è§é¢ä¼šï¼šäººå½¢æœºå™¨äººé€è¾¾å½•å–é€šçŸ¥ä¹¦]
        [å¾®ä¿¡è§†é¢‘å·ï¼šå¹¿å·å¹¿æ’­ç”µè§†å° - ä½ çš„æ¸¯ç§‘å¤§ï¼ˆå¹¿å·ï¼‰å½•å–é€šçŸ¥ä¹¦ï¼Œäººå½¢æœºå™¨äººåœ¨å¼€ç€â€œç–¾è·‘ â€é€æ¥ï¼]
      </li>
      <li>
        <span class="label label-info">07/2025</span>
        One paper accepted at ICCV 2025
        [<a href="https://gaussian-property.github.io/">Robot Manipulation with Physical-Property 3D Gaussians</a>]
        [<a href="https://arxiv.org/abs/2412.11258">paper</a>]
      </li>
      <li>
        <span class="label label-info">07/2025</span>
        ä½œä¸ºæ¼”è®²å˜‰å®¾å‚åŠ ä¸­å›½ç©ºé—´æ™ºèƒ½å¤§ä¼š @æ·±åœ³
        [<a href="https://mp.weixin.qq.com/s/mEddG_mbKFRarp0xetszGw?scene=1">ä¸­å›½ç©ºé—´æ™ºèƒ½å¤§ä¼š</a>]
      </li>
      <li>
        <span class="label label-info">06/2025</span>
        åœ¨æ¸¯ç§‘å¤§ï¼ˆå¹¿å·ï¼‰INNOTECHæ´»åŠ¨ç°åœºæ¼”ç¤ºæœ€æ–°locomotionã€äººå½¢æœºå™¨äººç§‘ç ”æˆæœã€‚
        Presented Live Demos at INNOTECH 2025@HKUST-GZ.
        [<a href="https://mp.weixin.qq.com/s/7XNbaXuVAi2WI2HRrt6GWg?scene=1">å¹¿å·å—æ²™å‘å¸ƒ</a>]
        [<a href="https://mp.weixin.qq.com/s/LvXlZdRN2Se95bAwxElNrw?scene=1">é¦™æ¸¯ç§‘æŠ€å¤§å­¦ï¼ˆå¹¿å·ï¼‰</a>]
        [<a href="https://www.stdaily.com/web/gdxw/2025-06/21/content_358337.html">ç§‘æŠ€æ—¥æŠ¥</a>]
      </li>
      <li>
        <span class="label label-info">06/2025</span>
        We are organizing a Social Navigation Competition at IROS 2025.
        æˆ‘ä»¬æ­£åœ¨ä¸¾åŠIROS 2025æœºå™¨äººç¤¾äº¤å¯¼èˆªæ¯”èµ›ï¼Œæ¬¢è¿å‚èµ›ï¼åæœˆæ­å·è§ï¼
        [<a href="https://robosense2025.github.io/#organizers">website æ¯”èµ›ç½‘ç«™</a>]
        [<a href="https://x.com/JunweilLiang/status/1930881503636127887">Tweet</a>]
        [<a href="https://www.xiaohongshu.com/discovery/item/68423902000000002202801a?source=webshare&xhsshare=pc_web&xsec_token=ABSybfxl1fFiqIS3y8k-ka_SSRaJMuSZbvX3mBfjhWbp8=&xsec_source=pc_share">å°çº¢ä¹¦å®£ä¼ </a>]
        [<a href="https://zhuanlan.zhihu.com/p/1914310707429737712">çŸ¥ä¹</a>]
      </li>
      <li>
        <span class="label label-info">06/2025</span>
        ä½œä¸ºé’å¹´è€å¸ˆå˜‰å®¾å‚åŠ VALSE 2025ä¼˜ç§€å­¦ç”Ÿè®ºå› @ç æµ·
        [<a href="https://mp.weixin.qq.com/s/E9PyUWEiNNuQ9MmCPmYfMw?scene=1">VALSE 2025ä¸“é¢˜è®ºå› | ä¼˜ç§€å­¦ç”Ÿè®ºå›ï¼šå‰æ²¿æŠ¥å‘Š+ä¸»é¢˜è¾©è®º+è±ªåå¯¼å¸ˆé¢å¯¹é¢</a>]
      </li>
      <li>
        <span class="label label-info">05/2025</span>
        The University has done a great profile for me.
        å­¦æ ¡å®˜åª’æŠ¥é“ï¼šâ€œæ–œæ é’å¹´â€è¿‡äº”å››ï¼šä»–ä»¬åœ¨æ¸¯ç§‘å¤§ï¼ˆå¹¿å·ï¼‰è§£é”Nç§äººç”Ÿå‰¯æœ¬ã€‚
        [<a href="https://mp.weixin.qq.com/s/39bhmntjBlD0xAbVOE5rVA">é¦™æ¸¯ç§‘æŠ€å¤§å­¦ï¼ˆå¹¿å·ï¼‰</a>]
      </li>
      <li>
        <span class="label label-info">04/2025</span>
        Present a talk on Embodied AI for the university.
        èšç„¦å‰æ²¿ï¼Œå…±è¯æœªæ¥ ï¼šæ¸¯ç§‘å¤§ï¼ˆå¹¿å·ï¼‰å…·èº«æ™ºèƒ½ç³»åˆ—è®ºå›é¦–åœºåœ¨å—æ²™IFCä¸¾è¡Œ
        [<a href="https://mp.weixin.qq.com/s/QCHTQZ0a67oARhaiu1n0xw">æ´»åŠ¨è®°å½•</a>]
      </li>
      <li>
        <span class="label label-info">04/2025</span>
        Serving as <span style="font-weight: bold;">Area Chair</span> for NeurIPS 2025.
      </li>
      <li>
        <span class="label label-info">03/2025</span>
        è·å¾—â€œAI100 é’å¹´å…ˆé”‹â€å¥– by MIT Technology Review
        [<a href="https://mp.weixin.qq.com/s/cxz0GDor7HqFuooPVyhu8Q">éº»çœç†å·¥ç§‘æŠ€è¯„è®º</a>]
        [<a href="https://mp.weixin.qq.com/s/pCatQkI2PY_utv_aPnJvQw">DeepTech</a>]
        [<a href="https://www.xiaohongshu.com/explore/67e7a1c6000000001c00e388?xsec_token=ABb9P73rz_xT3vtWs7U47WstMlvIGjtnAG_uEKjrx7XW4=&xsec_source=pc_user">å°çº¢ä¹¦</a>]
        [<a href="https://mp.weixin.qq.com/s/kcQl1R5zZHdC8FuZ8lb1jw">é¦™æ¸¯ç§‘æŠ€å¤§å­¦ä¿¡æ¯æ¢çº½</a>]
      </li>
      <li>
        <span class="label label-info">03/2025</span>
        Present a general Embodied AI introduction lecture for the university.
        åœ¨æ ¡å†…ä¸¾è¡Œäº†å…·èº«æ™ºèƒ½å‰æ²¿åº”ç”¨è®²åº§ï¼Œç»™å¤–è¡Œã€éæŠ€æœ¯äººå‘˜ä»‹ç»å…·èº«æ™ºèƒ½çš„ç›¸å…³æ¦‚å¿µä¸å‰æ²¿åº”ç”¨ï¼Œåå“ä¸é”™
        [<a href="https://mp.weixin.qq.com/s/7Qb_pvlvp3sX7uhWy56vWw">å®£ä¼ é¢„å‘Š</a>]
        [<a href="https://hkust-gz-edu-cn.zoom.us/rec/share/cGrIsDPi4THbWFz6wZgB17v1d601AgaJX7DwxFg3UvaHV0VkHBnLb6TgqkOYiHN3.w-SuMt6DszL4LrDd">è®²åº§å½•åƒ</a>]
        [<a href="https://docs.qq.com/sheet/DRHFqVXNmend5TVBJ?tab=000001">è®²åº§æ»¡æ„åº¦</a>]
      </li>
      <li>
        <span class="label label-info">02/2025</span>
        Two papers accepted at CVPR 2025.
        [<a href="https://jiaming-zhou.github.io/projects/HumanRobotAlign/">Robot Manipulation</a>]
        [<a href="https://seeground.github.io/">3D Visual Grounding</a>]
        [<a href="https://mp.weixin.qq.com/s/4D58UpJ7g3gmQbRyx0bCwQ">é‡å­ä½æŠ¥é“</a>]
        [<a href="https://mp.weixin.qq.com/s/LakS8zqiA5XunmEQykKCDw">æœºå™¨ä¹‹å¿ƒæŠ¥é“</a>][<a href="https://www.zhihu.com/question/13320524361/answer/130587951372">çŸ¥ä¹</a>]
        [<a href="https://mp.weixin.qq.com/s/Yozn7Qn1N7MHOSO0zxVVSw">æ™ºçŒ©çŒ©ç›´æ’­</a>]
      </li>
      <li>
        <span class="label label-info">01/2025</span>
        One paper accepted at ICRA 2025.
        [<a href="https://mp.weixin.qq.com/s/NT8Wk6N3S_mQwN_f39Arog">é‡å­ä½æŠ¥é“</a>]
        [<a href="https://mp.weixin.qq.com/s/iKeJJKG4QGReN0RYUvJKEA">å­¦æ ¡åª’ä½“æŠ¥é“</a>]
        [<a href="https://zeying-gong.github.io/projects/falcon/">Social Navigation</a>]
        [<a href="https://zhuanlan.zhihu.com/p/20569173162">çŸ¥ä¹</a>]
      </li>
      <li>
        <span class="label label-success">12/2024</span> æ¢“å¸†ã€ç‰¹ç«‹ã€ä½³æ˜è·å¾—2024æ·±åœ³æ™ºèƒ½æœºå™¨äººçµå·§æ‰‹å¤§èµ›ä¼˜èƒœå¥–
        [<a href="https://airs.cuhk.edu.cn/page/1226">æ·±åœ³å¸‚äººå·¥æ™ºèƒ½ä¸æœºå™¨äººç ”ç©¶é™¢ä¸¾åŠ</a>]
      </li>
      <li>
        <span class="label label-success">12/2024</span> è¿ªèªè·å¾—å¹¿æ±½é›†å›¢-æ¸¯ç§‘å¤§ï¼ˆå¹¿å·ï¼‰åˆ›æ–°äººæ‰å‘å±•å¥–å­¦é‡‘ (20ä¸‡)
        [<a href="https://mp.weixin.qq.com/s/kMudpT6nwoI2w5w4CByPjg">é¦™æ¸¯ç§‘æŠ€å¤§å­¦ï¼ˆå¹¿å·ï¼‰</a>]
      </li>
      <li>
        <span class="label label-info">11/2024</span>
        Presented "Towards General Service Embodied AI" at ARTS 2024. [<a href="https://mp.weixin.qq.com/s/dns8-rn1XAABrSdFJ-Aq7A">è‡ªä¸»æœºå™¨äººæŠ€æœ¯ç ”è®¨ä¼š</a>]
      </li>
      <li>
        <span class="label label-info">11/2024</span>
        My first PhD student, <a href="https://zgzxy001.github.io/">Xiaoyu Zhu</a>, has successfully defended her thesis and will graduate from CMU. Congrats to Xiaoyu! [<a href="https://www.cs.cmu.edu/calendar/178568916">Learning Generalizable Visual Representations Towards Novel Viewpoints, Scenes and Vocabularies</a>]
      </li>
      <li>
        <span class="label label-info">10/2024</span>
        Presented "Towards General Service Embodied AI" at Huawei and CCF-YOCSEF seminar.
      </li>
      <li>
        <span class="label label-info">09/2024</span>
        Presented "Towards General Service Embodied AI" at CCF/CSIG GAMES Seminar.
        [<a href="https://mp.weixin.qq.com/s/Vi7_VfHk8kE8XD_JEEQU3Q">ç¬¬ä¹å±Šè®¡ç®—æœºå›¾å½¢å­¦ä¸æ··åˆç°å®ç ”è®¨ä¼š</a>]
      </li>
      <li>
        <span class="label label-info">09/2024</span>
        One paper accepted at <a href="https://teleema.github.io/projects/Sigma_Agent/">CoRL 2024</a>.
      </li>
      <li>
        <span class="label label-info">09/2024</span>
        One paper accepted at NeurIPS 2024.
      </li>
      <li>
        <span class="label label-info">08/2024</span>
        One paper accepted at IROS 2024.
      </li>
      <li>
        <span class="label label-info">09/2024</span>
        è¢«é¦™æ¸¯ç§‘æŠ€å¤§å­¦ï¼ˆå¹¿å·ï¼‰å·¥ä¼šï¼Œè¯„ä¸ºç¬ƒå¿—æ˜Ÿï¼ˆå¼•é¢†æ˜Ÿï¼‰:D [<a href="https://mp.weixin.qq.com/s/lpoLauvc0th0oUw5RbHLdg">å­¦æ ¡æ–°é—»</a>]
      </li>

      <li>
        <span class="label label-info">09/2024</span>
        Presented "Towards General Service Embodied AI" at CAA's Seminar.
        [<a href="https://mp.weixin.qq.com/s/JNbY0mDoq4v6ugGL6LNiJQ">CAAä¸­å›½è‡ªåŠ¨åŒ–å­¦ä¼šäº‘è®²åº§</a>]
      </li>
      <li>
        <span class="label label-info">08/2024</span>
        Presented "Towards General Service Embodied AI" at CAAI's Embodied AI Seminar.
        [<a href="https://mp.weixin.qq.com/s/MkirFYZJYJu-tZgMRaSE-w">CAAIä¸­å›½äººå·¥æ™ºèƒ½å­¦ä¼šå…·èº«æ™ºèƒ½é’å¹´å­¦è€…ç ”è®¨ä¼šç¬¬äº”æœŸ</a>]
        [<a href="https://www.zhihu.com/zvideo/1815073015802781696">Video Recording</a>]
      </li>
      <li>
        <span class="label label-info">07/2024</span>
        Presented "Towards General Service Embodied AI" at the World AI Conference in Shanghai.
        [<a href="https://online2024.worldaic.com.cn/forumdetail?uuid=6e9fca0d377844e085fe7211f300ca19">WAIC</a>]
        [<a href="https://mp.weixin.qq.com/s/qZsHR-3adiDuku6ynwmcFg">è”æ±‡ç§‘æŠ€</a>]
      </li>
      <li>
        <span class="label label-info">07/2024</span>
        Two papers accepted at ECCV 2024.
      </li>
      <li>
        <span class="label label-info">05/2024</span>
        1 paper accepted at <a href="https://teleema.github.io/projects/SADE/sade.html">NAACL 2024</a>.
        1 paper accepted at ACL 2024. Main conference.
      </li>
      <li>
        <span class="label label-info">04/2024</span>
        My <a href="http://www.davidqiu.com/">PhD student Dicong Qiu</a> is reported by the university media.
        [<a href="https://mp.weixin.qq.com/s/pUpWrnDtIm5gK_ZWOWhxHg">HKUST(GZ)</a>]
        [<a href="https://mp.weixin.qq.com/s/WJxtyM2FRzOABdz_N35a6A">INFO Hub</a>]
        [<a href="https://mp.weixin.qq.com/s/5smO2jbZV7_OOXIecxWNPw">AI Thrust</a>]
      </li>
      <li>
        <span class="label label-info">02/2024</span>
        Serve as a panelist at the VALSE Embodied AI webinar.
        [<a href="https://mp.weixin.qq.com/s/788DpNUbjklH-unhznkewg?poc_token=HJlaAmaj-hqsfo_KezM6IrqyY7MsZZ5wYfVpxEyz">VALSE</a>]
        [<a href="https://live.bilibili.com/22300737">bilibili</a>]
      </li>
      <li>
        <span class="label label-info">02/2024</span> Co-organizing the <a href="https://sites.google.com/view/ieeecvf-cvpr2024-precognition">The 6th workshop on Precognition: Seeing through the Future</a> @CVPR 2024.
        [<a href="https://www.linkedin.com/posts/junweiliang_cvpr2024-computervision-visionforecasting-activity-7163724230201733120-ERWa/">Call For Papers</a>] [<a href="https://zhuanlan.zhihu.com/p/682320005">çŸ¥ä¹</a>] [<a href="https://www.xiaohongshu.com/explore/65cd76e400000000070055a8">å°çº¢ä¹¦</a>]
      </li>
      <li>
        <span class="label label-info">12/2023</span> Keynote speech at the CEII2023 Workshop
        [<a href="https://mp.weixin.qq.com/s/FRZ_r2BrvCz2Y7RreJdLgA">Schedule</a>]
      </li>
      <li>
        <span class="label label-info">10/2023</span> Co-organizing the Open-world Visual Perception Workshop (â€œå¼€æ”¾ä¸–ç•Œä¸‹çš„è§†è§‰æ„ŸçŸ¥å’Œå¢å¼ºâ€ä¸»é¢˜è®ºå›) @PRCV 2023
        [<a href="https://mp.weixin.qq.com/s/ib9aKBhQhoaAFqZB93F3wQ">Schedule</a>]
      </li>
      <li>
        <span class="label label-info">09/2023</span> Hosting HKUST AI Seminar series. Many thanks to the incoming speakers from around the world!
        [<a href="https://hkust-seminar.github.io/">Course Website</a>]
      </li>
      <li>
        <span class="label label-info">08/2023</span> HKUST-GZ PhD Summer Camp has started! Welcome!
        [<a href="https://www.kaggle.com/competitions/hkustgz-ai-summer-camp-junwei-p1">Project 1</a>]
        [<a href="https://www.kaggle.com/competitions/hkustgz-ai-summer-camp-junwei-p2">Project 2</a>]
      </li>
      <li>
        <span class="label label-info">07/2023</span> Attended a series of talks and events. No more traveling till fall!
        [<a href="https://www.xiaohongshu.com/explore/64a809f5000000001a011856">WAIC @Shanghai</a>]
        [<a href="https://www.xiaohongshu.com/explore/6483cb490000000013034466">RUC Seminar @Beijing</a>]
      </li>
      <li>
        <span class="label label-info">06/2023</span> The Precognition Workshop was successfully held at CVPR! Thanks to all the co-organizers and program committee members!
        [<a href="https://sites.google.com/view/ieeecvf-cvpr2023-precognition">Workshop Site</a>]
        [<a href="https://www.youtube.com/watch?v=Z3JhfOp0eGM">CVPR Workshop Recording</a>]
      </li>
      <li>
        <span class="label label-info">02/2023</span> One paper accepted by <span style="font-weight:bold">CVPR 2023</span>. Congrats to Xiaoyu!
      </li>
      <li>
        <span class="label label-info">02/2023</span> I'm teaching <a href="https://hkust-aiaa5032.github.io/">AIAA 5032 Foundations of Artificial Intelligence</a> and <a href="https://hkust-aiaa5036.github.io/">AIAA 5036 Autonomous AI</a> this semester at HKUST (Guangzhou).
      </li>
      <li>
        <!--<a href="https://www.linkedin.com/posts/khoa-luu-90900215_cvpr2023-activity-7009293998955655168-M4lu?utm_source=share&utm_medium=member_desktop">Precognition workshop</a>-->
        <span class="label label-info">01/2023</span> I am co-organizing the <a href="https://sites.google.com/view/ieeecvf-cvpr2023-precognition">The 5th workshop on Precognition: Seeing through the Future</a> @CVPR 2023. [<a href="https://www.linkedin.com/posts/junweiliang_cvpr2023-workshop-computervision-activity-7030466054787121152-NacF?utm_source=share&utm_medium=member_desktop">Call For Papers</a>] [<a href="https://zhuanlan.zhihu.com/p/603134088">çŸ¥ä¹</a>]
      </li>
      <li>
        <span class="label label-info">10/2022</span> Presented first-ever lecture at HKUST (Guangzhou).
        [<a href="https://www.youtube.com/watch?v=i2M9codDGes">AI Seminar</a>]
      </li>
      <li>
        <span class="label label-info">10/2022</span> <span style="font-weight: bold">Two</span> papers accepted at <span style="font-weight: bold">NeurIPS 2022</span>.
        [<a href="https://arxiv.org/abs/2209.12362">Multi-Action</a> (<a href="https://nips.cc/virtual/2022/spotlight/65262" style="color:red">Spotlight paper</a>, 3.7% acceptance rate, 384/10411)]
        [<a href="https://arxiv.org/abs/2209.13307">Video Retrieval</a>]
      </li>
      <li>
        <span class="label label-info">10/2022</span> Joined HKUST-GZ as a Tenure-Track Assistant Professor. Started an <a href="https://github.com/JunweiLiang/awesome_lists">awesome list</a> collection for TTAPs and PhD students.
      </li>
      <li>
        <span class="label label-info">09/2022</span> Invited to present at a young researcher forum by <a href="https://scholar.google.com/citations?user=qpBtpGsAAAAJ&hl=en">Prof. Xiaoou Tang</a> and <a href="https://www.shlab.org.cn/">Shanghai AI Lab</a>.
      </li>
      <li>
        <span class="label label-info">06/2022</span> Achieved <span style="font-weight:bold;">second-place</span> out of 150 teams on the <a href="https://arxiv.org/pdf/2204.10380.pdf">public leaderboard</a> of the Naturalist Driver Action Recognition Task - AI City Challenge @ CVPR 2022.
        [<a href="https://openaccess.thecvf.com/content/CVPR2022W/AICity/papers/Liang_Stargazer_A_Transformer-Based_Driver_Action_Detection_System_for_Intelligent_Transportation_CVPRW_2022_paper.pdf">CVPRW Paper</a>]
        [<a href="https://www.youtube.com/watch?v=u4CrNKt4P54">Presentation</a>] [<a href="https://github.com/JunweiLiang/aicity_action">Code and Model</a>]
      </li>
      <li>
        <span class="label label-info">10/2021</span> Published a <a href="https://www.techbeat.net/talk-info?id=588">research talk</a> at TechBeat.net on Pedestrian Trajectory Prediction. [<a href="https://www.techbeat.net/talk-info?id=588">å°†é—¨TechBeat</a>] [<a href="https://www.bilibili.com/video/BV1Y44y1x7nv/">Bç«™</a>]
      </li>
      <!--
      <li>
        <span class="label label-info">09/2021</span> Joined <span style="font-weight:bold">Tencent Youtu Lab</span> as a researcher.
      </li>
      -->
      <li>
        <span class="label label-info">08/2021</span> Received Doctoral Consortium Award at ICCV 2021, mentored by <a href="https://people.epfl.ch/alexandre.alahi?lang=en">Prof. Alexandre Alahi</a>.
      </li>
      <li>
        <span class="label label-info">08/2021</span> 1 paper accepted by <span style="font-weight:bold">ICCV 2021</span>.
      </li>
      <li>
        <span class="label label-info">08/2021</span> Our <a href="https://vera.cs.cmu.edu">VERA</a> system helps another major <span style="font-weight:bold">Washington Post</span> news report. [<a href="https://www.washingtonpost.com/world/interactive/2021/myanmar-crackdown-military-coup/">link</a>]
          <a href="https://www.washingtonpost.com/world/interactive/2021/myanmar-crackdown-military-coup/">
            <img class="press" src="resources/wapo.png"></img>
          </a>
      </li>
      <li>
        <span class="label label-info">07/2021</span> Successfully defended my Ph.D. thesis: From Recognition to Prediction: Analysis of Human Action and Trajectory Prediction in Video. [<a href="thesis/">link</a>]
      </li>
      <li>
        <span class="label label-info">04/2021</span> Featured in a <a href="https://www.washingtonpost.com/investigations/interactive/2021/dc-police-records-capitol-riot/">front-page news report</a> (04/15) by Washington Post using crowding counting technologies. [<a href="https://www.youtube.com/watch?v=rsQTY9083r8?t=1086">video</a>] [<a href="https://www.zhihu.com/zvideo/1366151651770834944">çŸ¥ä¹</a>]
        <a href="https://www.washingtonpost.com/investigations/interactive/2021/dc-police-records-capitol-riot/">
            <img class="press" src="resources/wapo.png"></img>
        </a>
      </li>
      <li>
        <span class="label label-info">01/2021</span> <span style="font-weight: bold;">Invited presentation</span> at ICPR'20 pattern forecasting workshop. [<a href="https://sites.google.com/di.uniroma1.it/patcast/program?authuser=0">link</a>]
      </li>
      <!--<li>
        [10/2020] Successfully proposed my PhD. thesis.
      </li>-->
      <li>
        <span class="label label-info">09/2020</span> We won the <a href="https://www.nist.gov/ctl/pscr/open-innovation-prize-challenges/past-prize-challenges/2020-automated-stream-analysis">Automated Streams Analysis for Public Safety Challenge</a> with a <a href="https://www.herox.com/ASAPS1/update/3483">$30k prize</a>.
      </li>
      <li>
        <span class="label label-info">08/2020</span> Our <a href="https://arxiv.org/abs/2006.16479">paper</a> has been accepted by WACV 2021 (one strong-accept) and <span style="font-weight:bold">reported by CMU news</span>:
        <a href="https://www.cmu.edu/news/stories/archives/2020/august/drones-hurricane-damage.html">
            <img class="press" src="resources/cmu.png"></img>
        </a>
      </li>
      <li>
        <span class="label label-info">08/2020</span> Analyzed videos for journalist from <span style="font-weight:bold">the Washington Post</span> on a <a href="https://www.washingtonpost.com/sports/2020/08/26/redskins-cheerleaders-video-daniel-snyder-washington/">major news</a>.
          <a href="https://www.washingtonpost.com/sports/2020/08/26/redskins-cheerleaders-video-daniel-snyder-washington/">
            <img class="press" src="resources/wapo.png"></img>
          </a>
      </li>
      <li>
        <span class="label label-info">07/2020</span> Awarded <a href="https://baijiahao.baidu.com/s?id=1671984902144018200&wfr=spider&for=pc"><span style="font-style: italic;">"AI Rising Star"</span></a> at the <a href="https://worldaic.com.cn/portal/en/index.html">World AI Conference</a>.
      </li>
      <li>
        <span class="label label-info">07/2020</span> <a href="https://precognition.team/next/simaug/"><span style="font-style: italic;">SimAug</span></a> paper accepted by <span style="font-weight:bold">ECCV 2020</span>.
      </li>
      <li>
        <span class="label label-info">06/2020</span> <a href="https://precognition.team/next/multiverse/index.html"><span style="font-style: italic;">Multiverse</span></a> (<span style="font-weight:bold">CVPR 2020</span>) code and dataset are released! [<a href="https://medium.com/@junweil/cvpr20-the-garden-of-forking-paths-towards-multi-future-trajectory-prediction-df23221dc9f8">blog</a>] [<a href="https://zhuanlan.zhihu.com/p/148343447">çŸ¥ä¹</a>] [<a href="https://github.com/JunweiLiang/Multiverse">code</a>]
      </li>
      <!--
      <li>
        <span class="label label-info">04/2020</span> A vision-based <a href="https://github.com/JunweiLiang/social-distancing-prediction"><span style="font-style: italic;">Social Distancing Early Forecasting</span></a> system is open-sourced</span>. Project received $6200 <a href="https://edu.google.com/programs/credits/research/">Google Cloud Research Grant</a>.
      </li>
      <li>
        <span class="label label-info">03/2020</span> <span style="font-weight:bold">Guest lecture</span> at CMU 11-775 class for grad students. [<a href="https://youtu.be/nbT7IIU8Sdc">Video</a>]
      </li>
      <li>
        [12/2019] <a href="https://precognition.team/next/multiverse/index.html"><span style="font-style: italic;">Multiverse</span></a> paper is out! Accepted by <span style="font-weight:bold">CVPR 2020</span>.
      </li>
      -->
      <li>
        <span class="label label-info">12/2019</span> Received <a href="http://scholarship.baidu.com/">Baidu Scholarship</a> (10 recipients globally).
        Press Coverage:
          <a href="http://news.ruc.edu.cn/archives/267603">
            <img class="press" src="resources/ruc.png"></img>
          </a>,
          <a href="http://m.china.com.cn/appshare/doc_1_20_1489589.html?from=groupmessage&isappinstalled=0">
            <img class="press" src="resources/china.png"></img>
          </a>,
          <a href="https://baijiahao.baidu.com/s?id=1654884571460145099&wfr=spider&for=pc">
            <img class="press" src="resources/baidu.png"></img>
          </a>,
          <a href="https://www.yanxishe.com/blogDetail/17504">
            <img class="press" src="resources/yanxishe.png"></img>
          </a>,
          <a href="http://app.bjheadline.com/8816/newshow.php?newsid=5514954&src=stream&typeid=20&uid=335186&did=16835adeb9fa457b8ec1f9b570dcc4b1&show=0&fSize=M&ver=2.6.3&ff=fz&mood=wx&from=groupmessage&isappinstalled=0">
            <img class="press" src="resources/bjheadline.png"></img>
          </a>
      </li>
      <li>
        <span class="label label-info">09/2019</span> Our <a href="https://vera.cs.cmu.edu/">Shooter Localization System</a> won <span style="font-weight:bold">Best Demo</span> award at <a href="https://cbmi2019.org/">CBMI2019</a>. [<a href="https://vera.cs.cmu.edu/" target="_blank">Project Site</a>]
        <br/>Press Coverage:
          <a href="https://www.cmu.edu/news/stories/archives/2019/november/system-locates-shooters-using-smartphone-video.html">
            <img class="press" src="resources/cmu.png"></img>
          </a>,
          <a href="https://pittsburgh.cbslocal.com/2019/11/20/cmu-develops-video-system-locate-mass-shooters/">
            <img class="press" src="resources/cbs.png"></img>
          </a>,
          <a href="https://www.post-gazette.com/business/tech-news/2019/11/20/Carnegie-Mellon-CMU-develops-cellphone-smartphone-video-system-location-shooter-triangulate/stories/201911200101">
            <img class="press" src="resources/post.png"></img>
          </a>,
          <a href="https://gizmodo.com/smartphone-videos-can-now-be-analyzed-and-used-to-pinpo-1839979803">
            <img class="press" src="resources/gizmodo.png"></img>
          </a>,
          <a href="https://www.dailymail.co.uk/sciencetech/article-7707501/Carnegie-Mellon-aims-end-pro-longed-massacres-locates-active-shooters.html">
            <img class="press" src="resources/dailymail.png"></img>
          </a>,
          <a href="https://www.techspot.com/news/82881-researchers-develop-system-can-pinpoint-shooter-location-using.html">
            <img class="press" src="resources/techspot.png"></img>
          </a>
      </li>
      <li>
        <span class="label label-info">06/2019</span> Presented Future Prediction paper at <span style="font-weight:bold">CVPR 2019</span>. It was reported by the media and it received <span style="font-weight:bold">30k+ views</span> in a week. <a href="https://precognition.team/next" target="_blank"><i title="Go to project page" class="icon-zoom-in"></i></a> [<a href="https://twitter.com/jcniebles/status/1141366303921303552" target="_blank">Tweets</a>]
      </li>
      <li><span class="label label-info">04/2019</span> Our CMU team's (INF & MUDSML) system achieved the <span style="font-weight:bold">best performance</span> on the <a href="https://actev.nist.gov/prizechallenge#tab_leaderboard" target="_blank">activity detection challenge</a> (<a href="resources/actev-prizechallenge-06-2019.png" target="_blank">Cached</a>) in surveillance videos hosted by NIST & IARPA. <!--The competitors include all other DIVA-funded teams from universities and companies as well as other strong participants from all over the world.--> We have released our code and model for Object Detection & Tracking <a href="https://github.com/JunweiLiang/Object_Detection_Tracking">here</a>. </li>
      <li><span class="label label-info">12/2018</span> <span style="font-weight:bold">MemexQA</span> paper accepted by <span style="font-weight:bold">TPAMI 2019</span>. <a href="https://precognition.team/memexqa" target="_blank"><i title="Go to project page" class="icon-zoom-in"></i></a></li>

      <li><span class="label label-info">06/2018</span> Presented MemexQA paper at <span style="font-weight:bold">CVPR 2018</span>. [<a href="https://youtu.be/TBOnKekODCI?t=1h11m29s" target="_blank">Spotlight Talk</a>]</li>
      <!--<li>[03/2017] Two papers accepted by ICASSP 2017.</li>
      <li>[02/2017] Two demo papers accepted by <span style="font-weight:bold">AAAI 2017</span>.</li>-->
      <li><span class="label label-info">11/2016</span> <span style="font-weight:bold">Best performer</span> in the NIST TRECVID 2016 Ad-hoc Video Search Challenge (no annotation track).</li>
      <!--<li>[02/2016] One oral paper accepted by <span style="font-weight:bold">IJCAI 2016</span>.</li>-->
    </ul>
  </div>

  <div class="title">
    <a class="title_link" id="awards" href="#awards">Awards</a>
  </div>


  <div class="content">

    <ul>
      <li>Best Paper Award, CoRL Workshop on Generalizable Priors for Robot Manipulation <div class="float-right">2025</div></li>
      <li>â€œAI100 é’å¹´å…ˆé”‹â€å¥– by MIT Technology Review <div class="float-right">2025</div></li>
      <li>ICCV Doctoral Consortium Award <div class="float-right">2021</div></li>
      <li>
        <a href="https://baijiahao.baidu.com/s?id=1671984902144018200&wfr=spider&for=pc"><span style="font-style: italic;">Rising Star</span></a> (äº‘å¸†å¥–-æ˜æ—¥ä¹‹æ˜Ÿ), World AI Conference <div class="float-right">2020</div>
      </li>
      <li>Baidu Scholarship (10 Ph.D. students worldwide) <div class="float-right">2019</div></li>
      <li>Winner, <a href="https://www.nist.gov/ctl/pscr/open-innovation-prize-challenges/past-prize-challenges/2020-automated-stream-analysis">Automated Streams Analysis for Public Safety Challenge</a> - $30k prize <div class="float-right">2020</div></li>
      <li>Best Demo Award at CBMI2019 <div class="float-right">2019</div></li>
      <li>Yahoo! Fellowship <div class="float-right">2016 - 2018</div></li>
      <li>Winner, TRECVID ActEV Challenge <div class="float-right">2019</div></li>
      <li>Winner, TRECVID Ad-hoc Video Search Challenge, no annotation track  <div class="float-right">2016</div></li>
      <li>CMU LTI Student Research Symposium Best Paper Honorable Mentions <div class="float-right">2018</div></li>
      <li>Google Cloud COVID-19 Research Grant - $6200 <div class="float-right">2020</div></li>
      <!--
      <li>CVPR, CES, IJCAI, ICASSP, NIST PSCR, NIST TRECVID student travel grants <div class="float-right">2016-2020</div></li>
      <li>Best Undergraduate Thesis (Top 5%) <div class="float-right">2015</div></li>
      <li>Second Prize, the National Undergraduates Computer Design Competition of China <div class="float-right">2014</div></li>
      <li>National Prize (Top 10%), National Undergraduates Innovation Project <div class="float-right">2013</div></li>
      -->
    </ul>
  </div>


  <div class="title">
    <a class="title_link" id="media" href="#media">Selected Media</a>
  </div>

  <div class="content">
    <ul>
      For more up-to-date media coverage, please visit my <a href="https://precognition.team/index.html#media">lab website.</a>
      <li>
        <span style="font-weight: bold">Washington Post.</span> <span style="font-style: italic;">How Shireen Abu Akleh was killed</span> (provided gunshot and shooter analysis), June 2022.
        [<a href="https://www.washingtonpost.com/investigations/interactive/2022/shireen-abu-akleh-death/?itid=lk_inline_manual_4/">Link</a>]
      </li>
      <li>
        <span style="font-weight: bold">Washington Post.</span> <span style="font-style: italic;">Anatomy of a crackdown</span> (provided gunshot and shooter analysis), August 25, 2021.
        [<a href="https://www.washingtonpost.com/world/interactive/2021/myanmar-crackdown-military-coup//">Link</a>]
      </li>
      <li>
        <span style="font-weight: bold">Washington Post.</span> <span style="font-style: italic;">17 requests for backup in 78 minutes</span> (provided crowd counting analysis), April 15, 2021.
        [<a href="https://www.washingtonpost.com/investigations/interactive/2021/dc-police-records-capitol-riot/">Link</a>]
      </li>
      <li>
        <span style="font-weight: bold">Carnegie Mellon University News.</span> <span style="font-style: italic;">Amateur Drone Videos Could Aid in Natural Disaster Damage Assessment</span>, August 28, 2020.
      </li>
      <li>
        <span style="font-weight: bold">AZO Robotics.</span> <span style="font-style: italic;">New AI System Helps Detect Damage Caused to Buildings by Hurricanes</span>, August 31, 2020.
      </li>
      <li>
        <span style="font-weight: bold">Washington Post.</span> <span style="font-style: italic;">Lewd cheerleader videos, sexist rules: Ex-employees decry Washingtonâ€™s NFL team workplace</span> (featured in the video analytics), August 26, 2020.
        [<a href="https://www.washingtonpost.com/sports/2020/08/26/redskins-cheerleaders-video-daniel-snyder-washington/">Link</a>]
      </li>
      <li>
        <span style="font-weight: bold">CBS.</span> <span style="font-style: italic;">Researchers At Carnegie Mellon University Develop Video System To Locate Mass Shooters Using Smartphones</span>, November 20, 2019.
      </li>
      <li>
        <span style="font-weight: bold">post-gazette.</span> <span style="font-style: italic;">CMU develops video system that can locate mass shooter</span>, November 20, 2019.
      </li>
      <li>
        <span style="font-weight: bold">GIZMODO.</span> <span style="font-style: italic;">Smartphone Videos Can Now Be Analyzed and Used to Pinpoint the Location of a Shooter</span>, November 21, 2019.
      </li>
      <li>
        <span style="font-weight: bold">DailyMail.</span> <span style="font-style: italic;">Active shooters can be located within minutes by new software that analyzes smartphone video from the scene and can even identify the type of gun</span>, November 20, 2019.
      </li>
      <li>
        <span style="font-weight: bold">Techspot.</span> <span style="font-style: italic;">Researchers develop system that can pinpoint a shooter's location using smartphone videos</span>, November 21, 2019.
      </li>
      <li>
        <span style="font-weight: bold">New York Times.</span> <span style="font-style: italic;">Who Killed the Kiev Protesters? A 3-D Model Holds the Clues</span> (featured in the video analytics), May 30, 2018.
      </li>
      <li>
        <span style="font-weight: bold">è¯»èŠ¯æœ¯.</span> <span style="font-style: italic;">å¡å†…åŸºæ¢…éš†å¤§å­¦æ¢ä¿Šå«ï¼šè§†é¢‘ä¸­è¡Œäººçš„å¤šç§æœªæ¥è½¨è¿¹é¢„æµ‹</span>, August, 2020.
      </li>
      <li>
        <span style="font-weight: bold">Baidu.</span> <span style="font-style: italic;">ä¹˜é£ç ´æµªçš„AIæŠ€æœ¯é’å¹´â€”â€”é¦–å±ŠWAICäº‘å¸†å¥–åå•å…¬å¸ƒ</span>, July 11, 2020.
      </li>
      <li>
        <span style="font-weight: bold">China.com.cn.</span> <span style="font-style: italic;">äººå¤§é«˜ç“´äººå·¥æ™ºèƒ½å­¦é™¢â€œé«˜å±‹å»ºç“´-é’å¹´è¯´â€é¦–æœŸå¼€è®²</span>, Jan 6, 2020.
      </li>
      <li>
        <span style="font-weight: bold">Baidu.</span> <span style="font-style: italic;">AIç•Œçš„ä¸­å›½åŠ›é‡ï¼ç™¾åº¦å¥–å­¦é‡‘åŠ©åŠ›ä¸­å›½AIäººæ‰ç»½æ”¾å…‰èŠ’ï¼</span>, Jan 5, 2020.
      </li>
      <li>
        <span style="font-weight: bold">é‡å­ä½.</span> <span style="font-style: italic;">æé£é£å›¢é˜Ÿé€ å‡ºâ€çª¥è§†æœªæ¥â€æ–°AI:å»å“ªå¹²å•¥ä¸€èµ·çŒœ, å‡†ç¡®ç‡å‹å€’è€å‰è¾ˆ</span>, received 30k+ views in a week, Feb 13, 2019.
      </li>
      <li>
        <span style="font-weight: bold">æœºå™¨ä¹‹å¿ƒ.</span> <span style="font-style: italic;">é‡è§æœªæ¥ï¼æé£é£ç­‰æå‡ºç«¯åˆ°ç«¯ç³»ç»ŸNexté¢„æµ‹æœªæ¥è·¯å¾„ä¸æ´»åŠ¨</span>, Feb 14, 2019.
      </li>
      <li>
        Aminer.cn, AI 2000 ranking (2019 - 2022).
        <br/>
        <img style="height: 400px" src="resources/ai_2000_2019_2022_rank.jpg"></img>
      </li>
    </ul>
  </div>


<!--

  <div class="title">
    <a class="title_link" id="exp" href="#exp">Research Experience</a>
  </div>

  <div class="content">
    <ul>
      <li>
        <span class="title">Researcher at Tencent Youtu Lab</span> <div class="float-right time">2021 - present</div>
        <div class="info">
          Work on large-scale video and language models and efficient long-term action detection applications.
        </div>
      </li>
      <li>
        <span class="title">Research Assistant at Carnegie Mellon University</span> <div class="float-right time">2015 - 2021</div>
        <div class="info">
          Worked on Large-scale Video Analysis and Retrieval. Studied unsupervised learning of video concept detectors from the Internet. Also participated in the development of event reconstruction tool.
          The project is for Synchronization and localization of noisy user-generated videos to reconstruct the event scene and timeline from unorganized social media videos, affiliates with CMU <a href="http://www.cmu.edu/chrs/" target="_blank">Center for Human Rights Science</a>.
          I'm also the major contributor to the government-funded projects: <a href="https://www.nist.gov/ctl/pscr/real-time-video-analytics-situation-awareness" target="_blank">PSCR by NIST</a> (2017-2020), <a href="https://www.iarpa.gov/index.php/research-programs/diva" target="_blank">DIVA by IARPA</a> (2017-2021) and <a href="https://www.iarpa.gov/index.php/research-programs/aladdin-video" target="_blank">ALADDIN by IARPA</a> (2017).
          Advised by <a href="https://scholar.google.com/citations?user=Py54GcEAAAAJ" target="_blank">Prof. Alexander Hauptmann</a>.
          [<a href="https://www.nist.gov/video/real-time-video-analytics-situation-awareness" target="_blank">PSCR 2018 presentation</a>, <a href="https://www.nist.gov/ctl/pscr/2019-stakeholder-meeting-analytics-sessions" target="_blank">2019</a>]
        </div>
      </li>
      <li>
        <span class="title">Research Intern at Google Cloud AI</span> <div class="float-right time">May 2020 - Aug 2020</div>
        <div class="info">
          Worked on viewpoint equivariant representation learning for activity recognition.
          Advised by <a href="https://scholar.google.com/citations?user=_lswGcYAAAAJ&hl=en" target="_blank">Dr. Ting Yu</a>, <a href="https://scholar.google.com/citations?user=vM1SktEAAAAJ&hl=en" target="_blank">Dr. Xuehan Xiong</a> and <a href="http://llcao.net/" target="_blank">Prof. Liangliang Cao</a>.
        </div>
      </li>
      <li>
        <span class="title">Research Intern at Google AI</span> <div class="float-right time">May 2019 - Aug 2019</div>
        <div class="info">
          Worked on future person activity and trajectory prediction in videos. Integrated research models to a Google Cloud product. Used 3D simulator (carla.org) to collect multi-modal future behavioral data.
          Advised by <a href="http://www.cs.cmu.edu/~lujiang/" target="_blank">Dr. Lu Jiang</a> and <a href="https://www.cs.ubc.ca/~murphyk/" target="_blank">Prof. Kevin Murphy</a>.
        </div>
      </li>
      <li>
        <span class="title">Student Researcher at Google Cloud AI</span> <div class="float-right time">May 2018 - Dec 2018</div>
        <div class="info">
          Worked on activity recognition and prediction in multi-perspective streaming videos. Studied principal computer vision and high-level semantic reasoning models for interperson and person-object interaction to help AI better understand human activities. Advised by <a href="http://www.cs.cmu.edu/~lujiang/" target="_blank">Dr. Lu Jiang</a> and <a href="http://www.niebles.net/" target="_blank">Prof. Juan Carlos Niebles</a>.
        </div>
      </li>
      <li>
        <span class="title">Research Assistant at Renmin University of China</span> <div class="float-right time">2013 - 2015</div>
        <div class="info">
          Studied semantic concept annotation on user-generated videos using audio. Participated HUAWEI semantic concept annotation of UGC videos grand challenge 2014 and ranked 3rd in the evaluation. Also worked on natural language description generation for images and videos with deep models. Ranked 1st in ImageCLEF 2015 â€œimage to sentenceâ€ subtask in the evaluation. Advised by <a href="https://scholar.google.com/citations?user=8UkYbCMAAAAJ&hl=zh-CN" target="_blank">Prof. Qin Jin</a>.
        </div>
      </li>
    </ul>
  </div>

  <div class="title">
    <a class="title_link" id="education" href="#education">Education</a>
  </div>

  <div class="content">
    <ul>
      <li>
        <span class="title">Ph.D. in Artificial Intelligence</span> <div class="float-right time">2017 - 2021</div>
        <div class="info">School of Computer Science, Carnegie Mellon University</div>
        <div class="info">Advisor: <a href="https://scholar.google.com/citations?user=Py54GcEAAAAJ&hl=en">Alexander Hauptmann</a></div>
        <div class="info">Thesis: From Recognition to Prediction: Analysis of Human Action and Trajectory Prediction in Video [<a href="thesis/">Link</a>]</div>
      </li>
      <li>
        <span class="title">M.S. in Artificial Intelligence</span> <div class="float-right time">2015 - 2017</div>
        <div class="info">School of Computer Science, Carnegie Mellon University</div>
        <div class="info">Advisor: <a href="https://scholar.google.com/citations?user=Py54GcEAAAAJ&hl=en">Alexander Hauptmann</a></div>
      </li>
      <li>
        <span class="title">B.S. in Computer Science</span> <div class="float-right time">2011 - 2015</div>
        <div class="info">School of Information, Renmin University of China</div>
        <div class="info">Advisor: <a href="https://scholar.google.com/citations?user=8UkYbCMAAAAJ&hl=en">Qin Jin</a></div>
      </li>
    </ul>
  </div>

  <div class="title">
    <a class="title_link" id="web" href="#web">Web App Experience</a>
  </div>
  <div class="content">
    <ul>
      <li>
        <span class="title"><a href="https://vera.cs.cmu.edu/">Shooter Localization from Social Media Videos</a></span>
        <div class="info">Widely reported by news media. Presented at CES 2020.</div>
      </li>
      <li>
        <span class="title"><a href="https://github.com/JunweiLiang/Lecture_Attendance_Management">Attendance Management System</a></span>
        <div class="info">
          Used within <a href="https://www.lti.cs.cmu.edu/">CMU LTI</a> for a course with over a hundred students every semester since 2017.
        </div>
      </li>
      <li>
        <span class="title">Major CMS websites for organizations</span>
        <div class="info"><a href="http://www.hillhouseacademy.com/">Hillhouse Academy</a>,  <a href="https://dasai.ruc.edu.cn/index.php/site/designer">Annual Computer Design Competition in China (>2000 users annually)</a> </div>
      </li>
    </ul>
  </div>
-->


</div>


<!--
	a Junwei Liang's production
	contact: junweiliang1114@gmail.com
-->
</body>
</html>

<html>
<head>
	<meta  http-equiv="Content-Type" content="text/html; charset=utf-8" />
	<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=1" />
	<link rel="stylesheet" type="text/css" href="utils/bootstrap.min.css"/>
  <script language="javascript" src="utils/jquery.min.js"></script>
	<script language="javascript" src="utils/bootstrap.min.js"></script>
  <!-- -->
  <link rel="stylesheet" type="text/css" href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css"/>
	<link rel="stylesheet" type="text/css" href="utils/cssReset.css"/>
  <link rel="stylesheet" type="text/css" href="utils/css_layout.css"/>
	<title>Prof. Junwei Liang / 梁俊卫</title>
  <!-- 搜索引擎优化stuff -->
	<meta name="description"
    content="Academic website for Junwei Liang. Dr. Junwei Liang is currently a tenure-track Assistant Professor at The Hong Kong University of Science and Technology (Guangzhou). He is also an affiliate assistant professor at HKUST computer science & engineering department. He was a senior researcher at Tencent Youtu Lab working on cutting-edge computer vision research and applications. Prior to that, he received his Ph.D. degree from Carnegie Mellon University, working with Prof. Alexander Hauptmann. He is the recipient of Baidu Scholarship and Yahoo Fellowship, and awarded Rising Star Award at the World AI Conference in 2020. He is the winner of several public safety video analysis competitions, including ASAPS and TRECVID ActEV. His work has helped and been reported by major news agencies like the Washington Post and New York Times. His research interests include human trajectory forecasting, action recognition, and large-scale computer vision and video analytics in general. His mission: develop AI technologies for social good.">
	<meta name="keywords" content="Junwei Liang,CMU,HKUST,HKUST-GZ,Professor,computer vision,PhD,梁俊卫,Carnegie Mellon University,The Hong Kong University of Science and Technology">

	<!-- Global site tag (gtag.js) - Google Analytics/ no use after 06/2024 -->
  <!--
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-156016426-1"></script>
  <script>
  	// for Google Analytics, for free!
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());
    gtag('config', 'UA-156016426-1');

  </script>
-->

</head>
<body>
<div id="sidebar">
  <img class='me' src="resources/me.jpeg"></img>
  <br/>
  <div class="info">
    <h2 class="name">Prof. Junwei Liang</h2>
    <h2 class="name_chinese">梁俊卫</h2>
    <h2 class="email">junweiliang1114@gmail.com</h2>
    <h2 class="email">junweiliang@hkust-gz.edu.cn</h2>
    <h2 class="email">HKUST (Guangzhou) / HKUST</h2>
    <h2 class="email">Office: E4-304</h2>
    <h2 class="link">
      <a style="font-size: 14px; color: yellow; font-weight: bold;" href="https://scholar.google.com/citations?hl=en&user=bMedjfUAAAAJ">Google Scholar</a>
    </h2>
    <h2 class="link">
      <a href="https://www.semanticscholar.org/author/Junwei-Liang/1915796">[Semantic Scholar]</a>
      <a href="https://www.researchgate.net/profile/Junwei_Liang3">[Research Gate]</a>
    </h2>
    <h2 class="link">
      <a href="https://github.com/JunweiLiang">[Github]</a>
      <a href="https://www.linkedin.com/in/junweiliang/">[LinkedIn]</a>
      <a href="https://www.youtube.com/channel/UC-z7ZWp8Rbu2xhxnbAL_bRQ">[Youtube]</a>
    </h2>
    <h2 class="link">
      <a href="https://www.zhihu.com/people/junwei-liang-50">[知乎]</a>
      <a href="https://www.xiaohongshu.com/user/profile/62c3a783000000001b02b099">[小红书]</a>
      <a href="https://twitter.com/JunweilLiang">[Twitter]</a>
    </h2>
    <h2 class="link">
      <a style="font-size: 14px; color: yellow; font-weight: bold;" href="https://precognition.team/">Precognition Lab</a>
    </h2>

    <!--
    <a class="quickLink" href="https://medium.com/@junweil">
      <img class='medium' style="" src="resources/medium.png"></img>
    </a>
    <a class="quickLink" href="https://dblp.org/pers/hd/l/Liang_0001:Junwei">
      <img class='dblp' style="height:20px" src="resources/dblp.png"></img>
    </a>
    <a class="quickLink" href="http://aminer.cn/profile/junwei-liang/562cb48c45cedb3398c9e13b">
      <img class='aminer' style="height:20px;width: 50px;margin-top:4px" src="resources/aminer.png"></img>
    </a>
    <a class="quickLink" href="https://g.co/kgs/gTWf5W">
      <img class='aminer' name="Google knowledge graph" style="height:30px;width: 30px;margin-top:0px" src="resources/gkg.png"></img>
    </a>-->

  </div>
  <div id="navigation">
    <a class="nav_item" href="./index.html">
      <i class="icon icon-home icon-white"></i> &nbsp; About
    </a>
    <a class="nav_item" href="./projects.html#projects">
      <i class="icon icon-th-large icon-white"></i> &nbsp; Projects
    </a>
    <a class="nav_item" href="./projects.html#publications">
      <i class="icon icon-file icon-white"></i> &nbsp; Publications
    </a>
    <a class="nav_item" href="./teaching.html#teaching">
      <i class="icon icon-user icon-white"></i> &nbsp; Teaching / Talks
    </a>
    <a class="nav_item" href="./index.html#awards">
      <i class="icon icon-bookmark icon-white"></i> &nbsp; Honors / Awards
    </a>
    <a class="nav_item" href="./index.html#media">
      <i class="icon icon-volume-up icon-white"></i> &nbsp; Selected Media
    </a>
    <a class="nav_item" href="./awesome.html">
      <i class="icon icon-list icon-white"></i> &nbsp; Awesome Lists
    </a>
    <a class="nav_item" href="./letter.html">
      <i class="icon icon-pencil icon-white"></i> &nbsp; Letter
    </a>

  </div>
</div>

<div id="main">
  <div class="title">
    <a class="title_link" id="bio" href="#bio">Bio</a>
    <!--<img src="https://visitor-badge.glitch.me/badge?page_id=JunweiLiang.JunweiLiang&right_color=green" alt=""/>-->
    <img src="https://vbr.wocr.tk/badge?page_id=JunweiLiang.JunweiLiang&right_color=green" alt=""/>

    <img id="logo" src="resources/hkustgz-logo.jpg"></img>
  </div>
  <div class="content">
    I am an Assistant Professor in the <a href="https://facultyprofiles.hkust-gz.edu.cn/thrust-faculties?code=10011A10000000000H28">AI thrust</a> at <a href="https://www.hkust-gz.edu.cn/">The Hong Kong University of Science and Technology (Guangzhou campus)</a>.
    I lead the <a href="https://precognition.team/">Precognition Lab</a>.
    I am also an Affiliate Assistant Professor at <a href="https://cse.hkust.edu.hk/admin/people/faculty/?c=affiliate_gz">HKUST computer science & engineering department</a>.
    I obtained my Ph.D. in 2021 from Carnegie Mellon University, advised by <a href="https://scholar.google.com/citations?user=Py54GcEAAAAJ&hl=en">Alexander Hauptmann</a>.

    <div class="linebreak"></div>
    <!--
    Prior to joining HKUST-GZ, I was a senior researcher at Tencent Youtu Lab working with <a href="https://scholar.google.com/citations?user=Ljk2BvIAAAAJ&hl=en">Chunhua Shen</a>. I completed my Ph.D. in 2021 from Carnegie Mellon University, advised by <a href="https://scholar.google.com/citations?user=Py54GcEAAAAJ&hl=en">Alexander Hauptmann</a>.
    My Ph.D. and research work were mostly funded by <a href="https://www.iarpa.gov/research-programs/diva">IARPA</a>, <a href="https://www.nist.gov/ctl/pscr/real-time-video-analytics-situation-awareness">NIST</a> and <a href="https://nsf.gov/awardsearch/showAward?AWD_ID=1650994&HistoricalAwards=false">NSF</a> grants.
    I was a research intern at Google AI multiple times and collaborated with <a href="https://scholar.google.com/citations?user=jIKjjSYAAAAJ&hl=en">Lu Jiang</a>, <a href="https://scholar.google.com/citations?user=S-hBSfIAAAAJ&hl=en">Liangliang Cao</a>, <a href="https://scholar.google.com/citations?user=feX1fWAAAAAJ">Jia Li</a> and <a href="https://scholar.google.com/citations?user=MxxZkEcAAAAJ&hl=en">Kevin Murphy</a>.
    I obtained my undergraduate degree from <a href="https://www.ruc.edu.cn/en">RUC</a>, advised by <a href="https://scholar.google.com/citations?user=8UkYbCMAAAAJ&hl=zh-CN">Qin Jin</a>.

    <div class="linebreak"></div>
    -->

    <!--
    <span style="color: red">I have multiple <strike>fully-funded Ph.D.</strike> and <strike>research assistant/intern positions available.</strike> (Our lab currently has around 10 people, and I want to ensure that everyone is producing meaningful research before I consider expanding the team.) </span> Please checkout potential <a href="./projects.html">projects</a> and <a href="./letter.html">this letter</a> if you are interested in joining our lab. 招生文: [<a href="https://zhuanlan.zhihu.com/p/562523740">知乎</a>] [<a href="https://www.xiaohongshu.com/discovery/item/6323ed9e00000000110142ad">小红书</a>]
    -->
    <!--
     <strike><span style="color: red">I am looking for one PhD. student to work on Embodied AI. Send me an email if you are interested and have two or more first-authored published papers at top conferences. </span></strike>-->
     My lab is at capacity now.
    I will look for PhD. students from MPhils/Undergrads of the university.
     Please checkout our <a href="https://precognition.team">lab resources</a> and <a href="./letter.html">this letter</a>. 招生文: [<a href="https://zhuanlan.zhihu.com/p/562523740">知乎</a>] [<a href="https://www.xiaohongshu.com/discovery/item/6323ed9e00000000110142ad">小红书</a>]

    <div class="linebreak"></div>

    <!--
    For fellow assistant professors and Ph.D. students, I have gathered some <span style="font-weight: bold">awesome lists</span> of resources that may be useful for your success. See <a href="./awesome.html">here</a> and feel free to contribute on Github. [<a href="https://wx.zsxq.com/mweb/views/topicdetail/topicdetail.html?topic_id=411558545282218&group_id=142181451122&inviter_id=582155511885254">被CVer盗了hhh</a>]
  -->


  </div>

  <div class="subtitle">
    <a class="title_link" id="group" href="#group">Precognition Lab</a>
    (Website: <a href="https://precognition.team">precognition.team</a>)
    -
    <a href="https://precognition.team/index.html#publications">Publications</a>
  </div>

  <div class="content">
    Our research lab, the Precognition Lab (智能感知与预测实验室), is interested in building human-level <span style="font-weight: bold;">Embodied AI</span> systems that can effectively perceive, reason, and interact with the real-world for the good of humans.
    Here is an up-to-date <a href="https://precognition.team/projects.html#roadmap">research roadmap</a>.
    Here are our on-going or finished <a href="./projects.html">research grants</a>.



    <div class="linebreak"></div>

    Our lab's computing resources include <b>36 RTX 3090/4090/L40 GPUs and a cluster of 24 A6000 GPUs</b> with a 100TB NAS. See <a href="https://www.linkedin.com/feed/update/urn:li:activity:7099940517966200832/">this post</a>.
    And we have multiple mobile platforms with robot arms and dex hands:

    <div class="linebreak"></div>

    <!-- <img src="resources/robot1.png" style="height:300px;margin:20px 30px 0 0"></img> -->
    <img src="resources/teleOp_dog_arm_tennis_stand_oncourt_noppl.gif" style="height:300px;margin:20px 10px 20px 0"></img>
    <img src="resources/g1tennis.gif" style="height:300px;margin:20px 10px 20px 0"></img>
    <img src="resources/robotdoghand_open_door.gif" style="height:300px;margin:20px 10px 20px 0"></img>
    <img src="resources/dex_handaliengo_grab_things.gif" style="height:300px;margin:20px 10px 20px 0"></img>
    <img src="resources/go2_arm.png" style="height:300px;margin:20px 10px 20px 0"></img>
    <img src="resources/robot3.png" style="height:300px;margin:20px 10px 20px 0"></img>

    <div class="linebreak"></div>

    If you want to meet, check out <a href="https://calendar.google.com/calendar/embed?src=junweiliang1114%40gmail.com&ctz=Asia%2FShanghai" target="_blank">my public calendar</a> first and propose a meeting via email.

  </div>

  <div class="title">
    <a class="title_link" id="news" href="#news">News</a>
  </div>

  <div class="content">
    <ul>
      <li>
        <span class="label label-info">03/2025</span>
        获得“AI100 青年先锋”奖 by MIT Technology Review
        [<a href="https://mp.weixin.qq.com/s/cxz0GDor7HqFuooPVyhu8Q">麻省理工科技评论</a>]
        [<a href="https://mp.weixin.qq.com/s/pCatQkI2PY_utv_aPnJvQw">DeepTech</a>]
        [<a href="https://www.xiaohongshu.com/explore/67e7a1c6000000001c00e388?xsec_token=ABb9P73rz_xT3vtWs7U47WstMlvIGjtnAG_uEKjrx7XW4=&xsec_source=pc_user">小红书</a>]
      </li>
      <li>
        <span class="label label-info">03/2025</span>
        Present a general Embodied AI introduction lecture for the university.
        在校内举行了具身智能前沿应用讲座，给外行、非技术人员介绍具身智能的相关概念与前沿应用，反响不错
        [<a href="https://mp.weixin.qq.com/s/7Qb_pvlvp3sX7uhWy56vWw">宣传预告</a>]
        [<a href="https://hkust-gz-edu-cn.zoom.us/rec/share/cGrIsDPi4THbWFz6wZgB17v1d601AgaJX7DwxFg3UvaHV0VkHBnLb6TgqkOYiHN3.w-SuMt6DszL4LrDd">讲座录像</a>]
        [<a href="https://docs.qq.com/sheet/DRHFqVXNmend5TVBJ?tab=000001">讲座满意度</a>]
      </li>
      <li>
        <span class="label label-info">02/2025</span>
        Two papers accepted at CVPR 2025.
        [<a href="https://jiaming-zhou.github.io/projects/HumanRobotAlign/">Robot Manipulation</a>]
        [<a href="https://seeground.github.io/">3D Visual Grounding</a>]
        [<a href="https://mp.weixin.qq.com/s/LakS8zqiA5XunmEQykKCDw">机器之心报道</a>][<a href="https://www.zhihu.com/question/13320524361/answer/130587951372">知乎</a>]
        [<a href="https://mp.weixin.qq.com/s/Yozn7Qn1N7MHOSO0zxVVSw">智猩猩直播</a>]
      </li>
      <li>
        <span class="label label-info">01/2025</span>
        One paper accepted at ICRA 2025.
        [<a href="https://zeying-gong.github.io/projects/falcon/">Social Navigation</a>]
        [<a href="https://zhuanlan.zhihu.com/p/20569173162">知乎</a>]
      </li>
      <li>
        <span class="label label-success">12/2024</span> 梓帆、特立、佳明获得2024深圳智能机器人灵巧手大赛优胜奖
        [<a href="https://airs.cuhk.edu.cn/page/1226">深圳市人工智能与机器人研究院举办</a>]
      </li>
      <li>
        <span class="label label-success">12/2024</span> 迪聪获得广汽集团-港科大（广州）创新人才发展奖学金 (20万)
        [<a href="https://mp.weixin.qq.com/s/kMudpT6nwoI2w5w4CByPjg">香港科技大学（广州）</a>]
      </li>
      <li>
        <span class="label label-info">11/2024</span>
        Presented "Towards General Service Embodied AI" at ARTS 2024. [<a href="https://mp.weixin.qq.com/s/dns8-rn1XAABrSdFJ-Aq7A">自主机器人技术研讨会</a>]
      </li>
      <li>
        <span class="label label-info">11/2024</span>
        My first PhD student, <a href="https://zgzxy001.github.io/">Xiaoyu Zhu</a>, has successfully defended her thesis and will graduate from CMU. Congrats to Xiaoyu! [<a href="https://www.cs.cmu.edu/calendar/178568916">Learning Generalizable Visual Representations Towards Novel Viewpoints, Scenes and Vocabularies</a>]
      </li>
      <li>
        <span class="label label-info">10/2024</span>
        Presented "Towards General Service Embodied AI" at Huawei and CCF-YOCSEF seminar.
      </li>
      <li>
        <span class="label label-info">09/2024</span>
        Presented "Towards General Service Embodied AI" at CCF/CSIG GAMES Seminar.
        [<a href="https://mp.weixin.qq.com/s/Vi7_VfHk8kE8XD_JEEQU3Q">第九届计算机图形学与混合现实研讨会</a>]
      </li>
      <li>
        <span class="label label-info">09/2024</span>
        One paper accepted at <a href="https://teleema.github.io/projects/Sigma_Agent/">CoRL 2024</a>.
      </li>
      <li>
        <span class="label label-info">09/2024</span>
        One paper accepted at NeurIPS 2024.
      </li>
      <li>
        <span class="label label-info">08/2024</span>
        One paper accepted at IROS 2024.
      </li>
      <li>
        <span class="label label-info">09/2024</span>
        被香港科技大学（广州）工会，评为笃志星（引领星）:D [<a href="https://mp.weixin.qq.com/s/lpoLauvc0th0oUw5RbHLdg">学校新闻</a>]
      </li>

      <li>
        <span class="label label-info">09/2024</span>
        Presented "Towards General Service Embodied AI" at CAA's Seminar.
        [<a href="https://mp.weixin.qq.com/s/JNbY0mDoq4v6ugGL6LNiJQ">CAA中国自动化学会云讲座</a>]
      </li>
      <li>
        <span class="label label-info">08/2024</span>
        Presented "Towards General Service Embodied AI" at CAAI's Embodied AI Seminar.
        [<a href="https://mp.weixin.qq.com/s/MkirFYZJYJu-tZgMRaSE-w">CAAI中国人工智能学会具身智能青年学者研讨会第五期</a>]
        [<a href="https://www.zhihu.com/zvideo/1815073015802781696">Video Recording</a>]
      </li>
      <li>
        <span class="label label-info">07/2024</span>
        Presented "Towards General Service Embodied AI" at the World AI Conference in Shanghai.
        [<a href="https://online2024.worldaic.com.cn/forumdetail?uuid=6e9fca0d377844e085fe7211f300ca19">WAIC</a>]
        [<a href="https://mp.weixin.qq.com/s/qZsHR-3adiDuku6ynwmcFg">联汇科技</a>]
      </li>
      <li>
        <span class="label label-info">07/2024</span>
        Two papers accepted at ECCV 2024.
      </li>
      <li>
        <span class="label label-info">05/2024</span>
        1 paper accepted at <a href="https://teleema.github.io/projects/SADE/sade.html">NAACL 2024</a>.
        1 paper accepted at ACL 2024. Main conference.
      </li>
      <li>
        <span class="label label-info">04/2024</span>
        My <a href="http://www.davidqiu.com/">PhD student Dicong Qiu</a> is reported by the university media.
        [<a href="https://mp.weixin.qq.com/s/pUpWrnDtIm5gK_ZWOWhxHg">HKUST(GZ)</a>]
        [<a href="https://mp.weixin.qq.com/s/WJxtyM2FRzOABdz_N35a6A">INFO Hub</a>]
        [<a href="https://mp.weixin.qq.com/s/5smO2jbZV7_OOXIecxWNPw">AI Thrust</a>]
      </li>
      <li>
        <span class="label label-info">02/2024</span>
        Serve as a panelist at the VALSE Embodied AI webinar.
        [<a href="https://mp.weixin.qq.com/s/788DpNUbjklH-unhznkewg?poc_token=HJlaAmaj-hqsfo_KezM6IrqyY7MsZZ5wYfVpxEyz">VALSE</a>]
        [<a href="https://live.bilibili.com/22300737">bilibili</a>]
      </li>
      <li>
        <span class="label label-info">02/2024</span> Co-organizing the <a href="https://sites.google.com/view/ieeecvf-cvpr2024-precognition">The 6th workshop on Precognition: Seeing through the Future</a> @CVPR 2024.
        [<a href="https://www.linkedin.com/posts/junweiliang_cvpr2024-computervision-visionforecasting-activity-7163724230201733120-ERWa/">Call For Papers</a>] [<a href="https://zhuanlan.zhihu.com/p/682320005">知乎</a>] [<a href="https://www.xiaohongshu.com/explore/65cd76e400000000070055a8">小红书</a>]
      </li>
      <li>
        <span class="label label-info">12/2023</span> Keynote speech at the CEII2023 Workshop
        [<a href="https://mp.weixin.qq.com/s/FRZ_r2BrvCz2Y7RreJdLgA">Schedule</a>]
      </li>
      <li>
        <span class="label label-info">10/2023</span> Co-organizing the Open-world Visual Perception Workshop (“开放世界下的视觉感知和增强”主题论坛) @PRCV 2023
        [<a href="https://mp.weixin.qq.com/s/ib9aKBhQhoaAFqZB93F3wQ">Schedule</a>]
      </li>
      <li>
        <span class="label label-info">09/2023</span> Hosting HKUST AI Seminar series. Many thanks to the incoming speakers from around the world!
        [<a href="https://hkust-seminar.github.io/">Course Website</a>]
      </li>
      <li>
        <span class="label label-info">08/2023</span> HKUST-GZ PhD Summer Camp has started! Welcome!
        [<a href="https://www.kaggle.com/competitions/hkustgz-ai-summer-camp-junwei-p1">Project 1</a>]
        [<a href="https://www.kaggle.com/competitions/hkustgz-ai-summer-camp-junwei-p2">Project 2</a>]
      </li>
      <li>
        <span class="label label-info">07/2023</span> Attended a series of talks and events. No more traveling till fall!
        [<a href="https://www.xiaohongshu.com/explore/64a809f5000000001a011856">WAIC @Shanghai</a>]
        [<a href="https://www.xiaohongshu.com/explore/6483cb490000000013034466">RUC Seminar @Beijing</a>]
      </li>
      <li>
        <span class="label label-info">06/2023</span> The Precognition Workshop was successfully held at CVPR! Thanks to all the co-organizers and program committee members!
        [<a href="https://sites.google.com/view/ieeecvf-cvpr2023-precognition">Workshop Site</a>]
        [<a href="https://www.youtube.com/watch?v=Z3JhfOp0eGM">CVPR Workshop Recording</a>]
      </li>
      <li>
        <span class="label label-info">02/2023</span> One paper accepted by <span style="font-weight:bold">CVPR 2023</span>. Congrats to Xiaoyu!
      </li>
      <li>
        <span class="label label-info">02/2023</span> I'm teaching <a href="https://hkust-aiaa5032.github.io/">AIAA 5032 Foundations of Artificial Intelligence</a> and <a href="https://hkust-aiaa5036.github.io/">AIAA 5036 Autonomous AI</a> this semester at HKUST (Guangzhou).
      </li>
      <li>
        <!--<a href="https://www.linkedin.com/posts/khoa-luu-90900215_cvpr2023-activity-7009293998955655168-M4lu?utm_source=share&utm_medium=member_desktop">Precognition workshop</a>-->
        <span class="label label-info">01/2023</span> I am co-organizing the <a href="https://sites.google.com/view/ieeecvf-cvpr2023-precognition">The 5th workshop on Precognition: Seeing through the Future</a> @CVPR 2023. [<a href="https://www.linkedin.com/posts/junweiliang_cvpr2023-workshop-computervision-activity-7030466054787121152-NacF?utm_source=share&utm_medium=member_desktop">Call For Papers</a>] [<a href="https://zhuanlan.zhihu.com/p/603134088">知乎</a>]
      </li>
      <li>
        <span class="label label-info">10/2022</span> Presented first-ever lecture at HKUST (Guangzhou).
        [<a href="https://www.youtube.com/watch?v=i2M9codDGes">AI Seminar</a>]
      </li>
      <li>
        <span class="label label-info">10/2022</span> <span style="font-weight: bold">Two</span> papers accepted at <span style="font-weight: bold">NeurIPS 2022</span>.
        [<a href="https://arxiv.org/abs/2209.12362">Multi-Action</a> (<a href="https://nips.cc/virtual/2022/spotlight/65262" style="color:red">Spotlight paper</a>, 3.7% acceptance rate, 384/10411)]
        [<a href="https://arxiv.org/abs/2209.13307">Video Retrieval</a>]
      </li>
      <li>
        <span class="label label-info">10/2022</span> Joined HKUST-GZ as a Tenure-Track Assistant Professor. Started an <a href="https://github.com/JunweiLiang/awesome_lists">awesome list</a> collection for TTAPs and PhD students.
      </li>
      <li>
        <span class="label label-info">09/2022</span> Invited to present at a young researcher forum by <a href="https://scholar.google.com/citations?user=qpBtpGsAAAAJ&hl=en">Prof. Xiaoou Tang</a> and <a href="https://www.shlab.org.cn/">Shanghai AI Lab</a>.
      </li>
      <li>
        <span class="label label-info">06/2022</span> Achieved <span style="font-weight:bold;">second-place</span> out of 150 teams on the <a href="https://arxiv.org/pdf/2204.10380.pdf">public leaderboard</a> of the Naturalist Driver Action Recognition Task - AI City Challenge @ CVPR 2022.
        [<a href="https://openaccess.thecvf.com/content/CVPR2022W/AICity/papers/Liang_Stargazer_A_Transformer-Based_Driver_Action_Detection_System_for_Intelligent_Transportation_CVPRW_2022_paper.pdf">CVPRW Paper</a>]
        [<a href="https://www.youtube.com/watch?v=u4CrNKt4P54">Presentation</a>] [<a href="https://github.com/JunweiLiang/aicity_action">Code and Model</a>]
      </li>
      <li>
        <span class="label label-info">10/2021</span> Published a <a href="https://www.techbeat.net/talk-info?id=588">research talk</a> at TechBeat.net on Pedestrian Trajectory Prediction. [<a href="https://www.techbeat.net/talk-info?id=588">将门TechBeat</a>] [<a href="https://www.bilibili.com/video/BV1Y44y1x7nv/">B站</a>]
      </li>
      <!--
      <li>
        <span class="label label-info">09/2021</span> Joined <span style="font-weight:bold">Tencent Youtu Lab</span> as a researcher.
      </li>
      -->
      <li>
        <span class="label label-info">08/2021</span> Received Doctoral Consortium Award at ICCV 2021, mentored by <a href="https://people.epfl.ch/alexandre.alahi?lang=en">Prof. Alexandre Alahi</a>.
      </li>
      <li>
        <span class="label label-info">08/2021</span> 1 paper accepted by <span style="font-weight:bold">ICCV 2021</span>.
      </li>
      <li>
        <span class="label label-info">08/2021</span> Our <a href="https://vera.cs.cmu.edu">VERA</a> system helps another major <span style="font-weight:bold">Washington Post</span> news report. [<a href="https://www.washingtonpost.com/world/interactive/2021/myanmar-crackdown-military-coup/">link</a>]
          <a href="https://www.washingtonpost.com/world/interactive/2021/myanmar-crackdown-military-coup/">
            <img class="press" src="resources/wapo.png"></img>
          </a>
      </li>
      <li>
        <span class="label label-info">07/2021</span> Successfully defended my Ph.D. thesis: From Recognition to Prediction: Analysis of Human Action and Trajectory Prediction in Video. [<a href="thesis/">link</a>]
      </li>
      <li>
        <span class="label label-info">04/2021</span> Featured in a <a href="https://www.washingtonpost.com/investigations/interactive/2021/dc-police-records-capitol-riot/">front-page news report</a> (04/15) by Washington Post using crowding counting technologies. [<a href="https://www.youtube.com/watch?v=rsQTY9083r8?t=1086">video</a>] [<a href="https://www.zhihu.com/zvideo/1366151651770834944">知乎</a>]
        <a href="https://www.washingtonpost.com/investigations/interactive/2021/dc-police-records-capitol-riot/">
            <img class="press" src="resources/wapo.png"></img>
        </a>
      </li>
      <li>
        <span class="label label-info">01/2021</span> <span style="font-weight: bold;">Invited presentation</span> at ICPR'20 pattern forecasting workshop. [<a href="https://sites.google.com/di.uniroma1.it/patcast/program?authuser=0">link</a>]
      </li>
      <!--<li>
        [10/2020] Successfully proposed my PhD. thesis.
      </li>-->
      <li>
        <span class="label label-info">09/2020</span> We won the <a href="https://www.nist.gov/ctl/pscr/open-innovation-prize-challenges/past-prize-challenges/2020-automated-stream-analysis">Automated Streams Analysis for Public Safety Challenge</a> with a <a href="https://www.herox.com/ASAPS1/update/3483">$30k prize</a>.
      </li>
      <li>
        <span class="label label-info">08/2020</span> Our <a href="https://arxiv.org/abs/2006.16479">paper</a> has been accepted by WACV 2021 (one strong-accept) and <span style="font-weight:bold">reported by CMU news</span>:
        <a href="https://www.cmu.edu/news/stories/archives/2020/august/drones-hurricane-damage.html">
            <img class="press" src="resources/cmu.png"></img>
        </a>
      </li>
      <li>
        <span class="label label-info">08/2020</span> Analyzed videos for journalist from <span style="font-weight:bold">the Washington Post</span> on a <a href="https://www.washingtonpost.com/sports/2020/08/26/redskins-cheerleaders-video-daniel-snyder-washington/">major news</a>.
          <a href="https://www.washingtonpost.com/sports/2020/08/26/redskins-cheerleaders-video-daniel-snyder-washington/">
            <img class="press" src="resources/wapo.png"></img>
          </a>
      </li>
      <li>
        <span class="label label-info">07/2020</span> Awarded <a href="https://baijiahao.baidu.com/s?id=1671984902144018200&wfr=spider&for=pc"><span style="font-style: italic;">"AI Rising Star"</span></a> at the <a href="https://worldaic.com.cn/portal/en/index.html">World AI Conference</a>.
      </li>
      <li>
        <span class="label label-info">07/2020</span> <a href="https://precognition.team/next/simaug/"><span style="font-style: italic;">SimAug</span></a> paper accepted by <span style="font-weight:bold">ECCV 2020</span>.
      </li>
      <li>
        <span class="label label-info">06/2020</span> <a href="https://precognition.team/next/multiverse/index.html"><span style="font-style: italic;">Multiverse</span></a> (<span style="font-weight:bold">CVPR 2020</span>) code and dataset are released! [<a href="https://medium.com/@junweil/cvpr20-the-garden-of-forking-paths-towards-multi-future-trajectory-prediction-df23221dc9f8">blog</a>] [<a href="https://zhuanlan.zhihu.com/p/148343447">知乎</a>] [<a href="https://github.com/JunweiLiang/Multiverse">code</a>]
      </li>
      <!--
      <li>
        <span class="label label-info">04/2020</span> A vision-based <a href="https://github.com/JunweiLiang/social-distancing-prediction"><span style="font-style: italic;">Social Distancing Early Forecasting</span></a> system is open-sourced</span>. Project received $6200 <a href="https://edu.google.com/programs/credits/research/">Google Cloud Research Grant</a>.
      </li>
      <li>
        <span class="label label-info">03/2020</span> <span style="font-weight:bold">Guest lecture</span> at CMU 11-775 class for grad students. [<a href="https://youtu.be/nbT7IIU8Sdc">Video</a>]
      </li>
      <li>
        [12/2019] <a href="https://precognition.team/next/multiverse/index.html"><span style="font-style: italic;">Multiverse</span></a> paper is out! Accepted by <span style="font-weight:bold">CVPR 2020</span>.
      </li>
      -->
      <li>
        <span class="label label-info">12/2019</span> Received <a href="http://scholarship.baidu.com/">Baidu Scholarship</a> (10 recipients globally).
        Press Coverage:
          <a href="http://news.ruc.edu.cn/archives/267603">
            <img class="press" src="resources/ruc.png"></img>
          </a>,
          <a href="http://m.china.com.cn/appshare/doc_1_20_1489589.html?from=groupmessage&isappinstalled=0">
            <img class="press" src="resources/china.png"></img>
          </a>,
          <a href="https://baijiahao.baidu.com/s?id=1654884571460145099&wfr=spider&for=pc">
            <img class="press" src="resources/baidu.png"></img>
          </a>,
          <a href="https://www.yanxishe.com/blogDetail/17504">
            <img class="press" src="resources/yanxishe.png"></img>
          </a>,
          <a href="http://app.bjheadline.com/8816/newshow.php?newsid=5514954&src=stream&typeid=20&uid=335186&did=16835adeb9fa457b8ec1f9b570dcc4b1&show=0&fSize=M&ver=2.6.3&ff=fz&mood=wx&from=groupmessage&isappinstalled=0">
            <img class="press" src="resources/bjheadline.png"></img>
          </a>
      </li>
      <li>
        <span class="label label-info">09/2019</span> Our <a href="https://vera.cs.cmu.edu/">Shooter Localization System</a> won <span style="font-weight:bold">Best Demo</span> award at <a href="https://cbmi2019.org/">CBMI2019</a>. [<a href="https://vera.cs.cmu.edu/" target="_blank">Project Site</a>]
        <br/>Press Coverage:
          <a href="https://www.cmu.edu/news/stories/archives/2019/november/system-locates-shooters-using-smartphone-video.html">
            <img class="press" src="resources/cmu.png"></img>
          </a>,
          <a href="https://pittsburgh.cbslocal.com/2019/11/20/cmu-develops-video-system-locate-mass-shooters/">
            <img class="press" src="resources/cbs.png"></img>
          </a>,
          <a href="https://www.post-gazette.com/business/tech-news/2019/11/20/Carnegie-Mellon-CMU-develops-cellphone-smartphone-video-system-location-shooter-triangulate/stories/201911200101">
            <img class="press" src="resources/post.png"></img>
          </a>,
          <a href="https://gizmodo.com/smartphone-videos-can-now-be-analyzed-and-used-to-pinpo-1839979803">
            <img class="press" src="resources/gizmodo.png"></img>
          </a>,
          <a href="https://www.dailymail.co.uk/sciencetech/article-7707501/Carnegie-Mellon-aims-end-pro-longed-massacres-locates-active-shooters.html">
            <img class="press" src="resources/dailymail.png"></img>
          </a>,
          <a href="https://www.techspot.com/news/82881-researchers-develop-system-can-pinpoint-shooter-location-using.html">
            <img class="press" src="resources/techspot.png"></img>
          </a>
      </li>
      <li>
        <span class="label label-info">06/2019</span> Presented Future Prediction paper at <span style="font-weight:bold">CVPR 2019</span>. It was reported by the media and it received <span style="font-weight:bold">30k+ views</span> in a week. <a href="https://precognition.team/next" target="_blank"><i title="Go to project page" class="icon-zoom-in"></i></a> [<a href="https://twitter.com/jcniebles/status/1141366303921303552" target="_blank">Tweets</a>]
      </li>
      <li><span class="label label-info">04/2019</span> Our CMU team's (INF & MUDSML) system achieved the <span style="font-weight:bold">best performance</span> on the <a href="https://actev.nist.gov/prizechallenge#tab_leaderboard" target="_blank">activity detection challenge</a> (<a href="resources/actev-prizechallenge-06-2019.png" target="_blank">Cached</a>) in surveillance videos hosted by NIST & IARPA. <!--The competitors include all other DIVA-funded teams from universities and companies as well as other strong participants from all over the world.--> We have released our code and model for Object Detection & Tracking <a href="https://github.com/JunweiLiang/Object_Detection_Tracking">here</a>. </li>
      <li><span class="label label-info">12/2018</span> <span style="font-weight:bold">MemexQA</span> paper accepted by <span style="font-weight:bold">TPAMI 2019</span>. <a href="https://precognition.team/memexqa" target="_blank"><i title="Go to project page" class="icon-zoom-in"></i></a></li>

      <li><span class="label label-info">06/2018</span> Presented MemexQA paper at <span style="font-weight:bold">CVPR 2018</span>. [<a href="https://youtu.be/TBOnKekODCI?t=1h11m29s" target="_blank">Spotlight Talk</a>]</li>
      <!--<li>[03/2017] Two papers accepted by ICASSP 2017.</li>
      <li>[02/2017] Two demo papers accepted by <span style="font-weight:bold">AAAI 2017</span>.</li>-->
      <li><span class="label label-info">11/2016</span> <span style="font-weight:bold">Best performer</span> in the NIST TRECVID 2016 Ad-hoc Video Search Challenge (no annotation track).</li>
      <!--<li>[02/2016] One oral paper accepted by <span style="font-weight:bold">IJCAI 2016</span>.</li>-->
    </ul>
  </div>

  <div class="title">
    <a class="title_link" id="awards" href="#awards">Awards</a>
  </div>


  <div class="content">

    <ul>
      <li>“AI100 青年先锋”奖 by MIT Technology Review <div class="float-right">2025</div></li>
      <li>ICCV Doctoral Consortium Award <div class="float-right">2021</div></li>
      <li>
        <a href="https://baijiahao.baidu.com/s?id=1671984902144018200&wfr=spider&for=pc"><span style="font-style: italic;">Rising Star</span></a> (云帆奖-明日之星), World AI Conference <div class="float-right">2020</div>
      </li>
      <li>Baidu Scholarship (10 Ph.D. students worldwide) <div class="float-right">2019</div></li>
      <li>Winner, <a href="https://www.nist.gov/ctl/pscr/open-innovation-prize-challenges/past-prize-challenges/2020-automated-stream-analysis">Automated Streams Analysis for Public Safety Challenge</a> - $30k prize <div class="float-right">2020</div></li>
      <li>Best Demo Award at CBMI2019 <div class="float-right">2019</div></li>
      <li>Yahoo! Fellowship <div class="float-right">2016 - 2018</div></li>
      <li>Winner, TRECVID ActEV Challenge <div class="float-right">2019</div></li>
      <li>Winner, TRECVID Ad-hoc Video Search Challenge, no annotation track  <div class="float-right">2016</div></li>
      <li>CMU LTI Student Research Symposium Best Paper Honorable Mentions <div class="float-right">2018</div></li>
      <li>Google Cloud COVID-19 Research Grant - $6200 <div class="float-right">2020</div></li>
      <!--
      <li>CVPR, CES, IJCAI, ICASSP, NIST PSCR, NIST TRECVID student travel grants <div class="float-right">2016-2020</div></li>
      <li>Best Undergraduate Thesis (Top 5%) <div class="float-right">2015</div></li>
      <li>Second Prize, the National Undergraduates Computer Design Competition of China <div class="float-right">2014</div></li>
      <li>National Prize (Top 10%), National Undergraduates Innovation Project <div class="float-right">2013</div></li>
      -->
    </ul>
  </div>


  <div class="title">
    <a class="title_link" id="media" href="#media">Selected Media</a>
  </div>

  <div class="content">
    <ul>
      For more up-to-date media coverage, please visit my <a href="https://precognition.team/index.html#media">lab website.</a>
      <li>
        <span style="font-weight: bold">Washington Post.</span> <span style="font-style: italic;">How Shireen Abu Akleh was killed</span> (provided gunshot and shooter analysis), June 2022.
        [<a href="https://www.washingtonpost.com/investigations/interactive/2022/shireen-abu-akleh-death/?itid=lk_inline_manual_4/">Link</a>]
      </li>
      <li>
        <span style="font-weight: bold">Washington Post.</span> <span style="font-style: italic;">Anatomy of a crackdown</span> (provided gunshot and shooter analysis), August 25, 2021.
        [<a href="https://www.washingtonpost.com/world/interactive/2021/myanmar-crackdown-military-coup//">Link</a>]
      </li>
      <li>
        <span style="font-weight: bold">Washington Post.</span> <span style="font-style: italic;">17 requests for backup in 78 minutes</span> (provided crowd counting analysis), April 15, 2021.
        [<a href="https://www.washingtonpost.com/investigations/interactive/2021/dc-police-records-capitol-riot/">Link</a>]
      </li>
      <li>
        <span style="font-weight: bold">Carnegie Mellon University News.</span> <span style="font-style: italic;">Amateur Drone Videos Could Aid in Natural Disaster Damage Assessment</span>, August 28, 2020.
      </li>
      <li>
        <span style="font-weight: bold">AZO Robotics.</span> <span style="font-style: italic;">New AI System Helps Detect Damage Caused to Buildings by Hurricanes</span>, August 31, 2020.
      </li>
      <li>
        <span style="font-weight: bold">Washington Post.</span> <span style="font-style: italic;">Lewd cheerleader videos, sexist rules: Ex-employees decry Washington’s NFL team workplace</span> (featured in the video analytics), August 26, 2020.
        [<a href="https://www.washingtonpost.com/sports/2020/08/26/redskins-cheerleaders-video-daniel-snyder-washington/">Link</a>]
      </li>
      <li>
        <span style="font-weight: bold">CBS.</span> <span style="font-style: italic;">Researchers At Carnegie Mellon University Develop Video System To Locate Mass Shooters Using Smartphones</span>, November 20, 2019.
      </li>
      <li>
        <span style="font-weight: bold">post-gazette.</span> <span style="font-style: italic;">CMU develops video system that can locate mass shooter</span>, November 20, 2019.
      </li>
      <li>
        <span style="font-weight: bold">GIZMODO.</span> <span style="font-style: italic;">Smartphone Videos Can Now Be Analyzed and Used to Pinpoint the Location of a Shooter</span>, November 21, 2019.
      </li>
      <li>
        <span style="font-weight: bold">DailyMail.</span> <span style="font-style: italic;">Active shooters can be located within minutes by new software that analyzes smartphone video from the scene and can even identify the type of gun</span>, November 20, 2019.
      </li>
      <li>
        <span style="font-weight: bold">Techspot.</span> <span style="font-style: italic;">Researchers develop system that can pinpoint a shooter's location using smartphone videos</span>, November 21, 2019.
      </li>
      <li>
        <span style="font-weight: bold">New York Times.</span> <span style="font-style: italic;">Who Killed the Kiev Protesters? A 3-D Model Holds the Clues</span> (featured in the video analytics), May 30, 2018.
      </li>
      <li>
        <span style="font-weight: bold">读芯术.</span> <span style="font-style: italic;">卡内基梅隆大学梁俊卫：视频中行人的多种未来轨迹预测</span>, August, 2020.
      </li>
      <li>
        <span style="font-weight: bold">Baidu.</span> <span style="font-style: italic;">乘风破浪的AI技术青年——首届WAIC云帆奖名单公布</span>, July 11, 2020.
      </li>
      <li>
        <span style="font-weight: bold">China.com.cn.</span> <span style="font-style: italic;">人大高瓴人工智能学院“高屋建瓴-青年说”首期开讲</span>, Jan 6, 2020.
      </li>
      <li>
        <span style="font-weight: bold">Baidu.</span> <span style="font-style: italic;">AI界的中国力量！百度奖学金助力中国AI人才绽放光芒！</span>, Jan 5, 2020.
      </li>
      <li>
        <span style="font-weight: bold">量子位.</span> <span style="font-style: italic;">李飞飞团队造出”窥视未来”新AI:去哪干啥一起猜, 准确率压倒老前辈</span>, received 30k+ views in a week, Feb 13, 2019.
      </li>
      <li>
        <span style="font-weight: bold">机器之心.</span> <span style="font-style: italic;">遇见未来！李飞飞等提出端到端系统Next预测未来路径与活动</span>, Feb 14, 2019.
      </li>
      <li>
        Aminer.cn, AI 2000 ranking (2019 - 2022).
        <br/>
        <img style="height: 400px" src="resources/ai_2000_2019_2022_rank.jpg"></img>
      </li>
    </ul>
  </div>


<!--

  <div class="title">
    <a class="title_link" id="exp" href="#exp">Research Experience</a>
  </div>

  <div class="content">
    <ul>
      <li>
        <span class="title">Researcher at Tencent Youtu Lab</span> <div class="float-right time">2021 - present</div>
        <div class="info">
          Work on large-scale video and language models and efficient long-term action detection applications.
        </div>
      </li>
      <li>
        <span class="title">Research Assistant at Carnegie Mellon University</span> <div class="float-right time">2015 - 2021</div>
        <div class="info">
          Worked on Large-scale Video Analysis and Retrieval. Studied unsupervised learning of video concept detectors from the Internet. Also participated in the development of event reconstruction tool.
          The project is for Synchronization and localization of noisy user-generated videos to reconstruct the event scene and timeline from unorganized social media videos, affiliates with CMU <a href="http://www.cmu.edu/chrs/" target="_blank">Center for Human Rights Science</a>.
          I'm also the major contributor to the government-funded projects: <a href="https://www.nist.gov/ctl/pscr/real-time-video-analytics-situation-awareness" target="_blank">PSCR by NIST</a> (2017-2020), <a href="https://www.iarpa.gov/index.php/research-programs/diva" target="_blank">DIVA by IARPA</a> (2017-2021) and <a href="https://www.iarpa.gov/index.php/research-programs/aladdin-video" target="_blank">ALADDIN by IARPA</a> (2017).
          Advised by <a href="https://scholar.google.com/citations?user=Py54GcEAAAAJ" target="_blank">Prof. Alexander Hauptmann</a>.
          [<a href="https://www.nist.gov/video/real-time-video-analytics-situation-awareness" target="_blank">PSCR 2018 presentation</a>, <a href="https://www.nist.gov/ctl/pscr/2019-stakeholder-meeting-analytics-sessions" target="_blank">2019</a>]
        </div>
      </li>
      <li>
        <span class="title">Research Intern at Google Cloud AI</span> <div class="float-right time">May 2020 - Aug 2020</div>
        <div class="info">
          Worked on viewpoint equivariant representation learning for activity recognition.
          Advised by <a href="https://scholar.google.com/citations?user=_lswGcYAAAAJ&hl=en" target="_blank">Dr. Ting Yu</a>, <a href="https://scholar.google.com/citations?user=vM1SktEAAAAJ&hl=en" target="_blank">Dr. Xuehan Xiong</a> and <a href="http://llcao.net/" target="_blank">Prof. Liangliang Cao</a>.
        </div>
      </li>
      <li>
        <span class="title">Research Intern at Google AI</span> <div class="float-right time">May 2019 - Aug 2019</div>
        <div class="info">
          Worked on future person activity and trajectory prediction in videos. Integrated research models to a Google Cloud product. Used 3D simulator (carla.org) to collect multi-modal future behavioral data.
          Advised by <a href="http://www.cs.cmu.edu/~lujiang/" target="_blank">Dr. Lu Jiang</a> and <a href="https://www.cs.ubc.ca/~murphyk/" target="_blank">Prof. Kevin Murphy</a>.
        </div>
      </li>
      <li>
        <span class="title">Student Researcher at Google Cloud AI</span> <div class="float-right time">May 2018 - Dec 2018</div>
        <div class="info">
          Worked on activity recognition and prediction in multi-perspective streaming videos. Studied principal computer vision and high-level semantic reasoning models for interperson and person-object interaction to help AI better understand human activities. Advised by <a href="http://www.cs.cmu.edu/~lujiang/" target="_blank">Dr. Lu Jiang</a> and <a href="http://www.niebles.net/" target="_blank">Prof. Juan Carlos Niebles</a>.
        </div>
      </li>
      <li>
        <span class="title">Research Assistant at Renmin University of China</span> <div class="float-right time">2013 - 2015</div>
        <div class="info">
          Studied semantic concept annotation on user-generated videos using audio. Participated HUAWEI semantic concept annotation of UGC videos grand challenge 2014 and ranked 3rd in the evaluation. Also worked on natural language description generation for images and videos with deep models. Ranked 1st in ImageCLEF 2015 “image to sentence” subtask in the evaluation. Advised by <a href="https://scholar.google.com/citations?user=8UkYbCMAAAAJ&hl=zh-CN" target="_blank">Prof. Qin Jin</a>.
        </div>
      </li>
    </ul>
  </div>

  <div class="title">
    <a class="title_link" id="education" href="#education">Education</a>
  </div>

  <div class="content">
    <ul>
      <li>
        <span class="title">Ph.D. in Artificial Intelligence</span> <div class="float-right time">2017 - 2021</div>
        <div class="info">School of Computer Science, Carnegie Mellon University</div>
        <div class="info">Advisor: <a href="https://scholar.google.com/citations?user=Py54GcEAAAAJ&hl=en">Alexander Hauptmann</a></div>
        <div class="info">Thesis: From Recognition to Prediction: Analysis of Human Action and Trajectory Prediction in Video [<a href="thesis/">Link</a>]</div>
      </li>
      <li>
        <span class="title">M.S. in Artificial Intelligence</span> <div class="float-right time">2015 - 2017</div>
        <div class="info">School of Computer Science, Carnegie Mellon University</div>
        <div class="info">Advisor: <a href="https://scholar.google.com/citations?user=Py54GcEAAAAJ&hl=en">Alexander Hauptmann</a></div>
      </li>
      <li>
        <span class="title">B.S. in Computer Science</span> <div class="float-right time">2011 - 2015</div>
        <div class="info">School of Information, Renmin University of China</div>
        <div class="info">Advisor: <a href="https://scholar.google.com/citations?user=8UkYbCMAAAAJ&hl=en">Qin Jin</a></div>
      </li>
    </ul>
  </div>

  <div class="title">
    <a class="title_link" id="web" href="#web">Web App Experience</a>
  </div>
  <div class="content">
    <ul>
      <li>
        <span class="title"><a href="https://vera.cs.cmu.edu/">Shooter Localization from Social Media Videos</a></span>
        <div class="info">Widely reported by news media. Presented at CES 2020.</div>
      </li>
      <li>
        <span class="title"><a href="https://github.com/JunweiLiang/Lecture_Attendance_Management">Attendance Management System</a></span>
        <div class="info">
          Used within <a href="https://www.lti.cs.cmu.edu/">CMU LTI</a> for a course with over a hundred students every semester since 2017.
        </div>
      </li>
      <li>
        <span class="title">Major CMS websites for organizations</span>
        <div class="info"><a href="http://www.hillhouseacademy.com/">Hillhouse Academy</a>,  <a href="https://dasai.ruc.edu.cn/index.php/site/designer">Annual Computer Design Competition in China (>2000 users annually)</a> </div>
      </li>
    </ul>
  </div>
-->


</div>


<!--
	a Junwei Liang's production
	contact: junweiliang1114@gmail.com
-->
</body>
</html>
